+++
date = '2025-11-17T12:42:22+08:00'
draft = false
title = 'Transformer'
+++

# Transformer

å¤§è¯­è¨€æ¨¡å‹è¦å…·æœ‰å¤æ‚ç†è§£ä»¥åŠç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸æ˜¯ä¸ºç‰¹å®šçš„è¯­è¨€ä»»åŠ¡æ‰€è®¾è®¡è€Œæ˜¯å…·æœ‰æ›´å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹çš„æˆåŠŸå½’å› äºTransformeræ¶æ„ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„æµ·é‡æ•°æ®ã€‚

é€šè¿‡ç¼–å†™ä»£ç åŸºäºTransformeræ¶æ„å®ç°ç±»ä¼¼ChatGPTçš„å¤§è¯­è¨€æ¨¡å‹ã€‚

å¤§è¯­è¨€æ¨¡å‹çš„å¤§æ—¢æŒ‡å‚æ•°é‡è§„æ¨¡ï¼ŒåˆæŒ‡æµ·é‡æ•°æ®ã€‚

Transformerå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯å®ƒèƒ½å¤Ÿé€‰æ‹©æ€§çš„å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚å…³é”®ç»„ä»¶æ˜¯self-attentionï¼Œèƒ½å¤Ÿè¡¡é‡è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªtokenç›¸å¯¹äºå…¶ä»–tokençš„ç›¸å¯¹é‡è¦æ€§ã€‚

å¤§è¯­è¨€æ¨¡å‹çš„æ„å»ºåŒ…å«pretraining and fine-tuningä¸¤ä¸ªé˜¶æ®µã€‚pretrainingé˜¶æ®µæ˜¯åœ¨æµ·é‡unlabeled text dataä¸Šè¿›è¡Œself-supervised learningï¼Œå»å­¦åˆ°ä¸€äº›general representionã€‚fine-tuningæ˜¯train on labeled dataã€‚

Transformeråˆ†ä¸ºencoderå’Œdecoderéƒ¨åˆ†ã€‚encoderè´Ÿè´£å°†è¾“å…¥åºåˆ—ç¼–ç æˆa series of numerical representations or vectors that capture the contextual information of the inputï¼Œè§£ç å™¨æ ¹æ®å½“å‰çš„these encoded vectorsä»¥åŠå½“å‰è¾“å…¥å®Œæˆä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ã€‚

å®ç°é¢„è®­ç»ƒçš„ä»£ç ã€å¤ç”¨å…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹

The next-word prediction taskæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦ä¸ºè®­ç»ƒæ•°æ®æä¾›æ ‡ç­¾ï¼Œä»–åˆ©ç”¨æ•°æ®è‡ªèº«çš„ç»“æ„ã€‚ç”¨æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªè¯ä½œä¸ºè¦è®­ç»ƒçš„æ ‡ç­¾ã€‚

Autoregressive modelsæ•´åˆæ—©å…ˆçš„è¾“å‡ºä½œä¸ºæœªæ¥é¢„æµ‹çš„è¾“å…¥ã€‚

original transformer çš„encoderå’Œdecoderé‡å¤6æ¬¡ã€‚GPT-3æœ‰96å±‚transformer layers ä»¥åŠ175bçš„parametersã€‚

emergent behaviorï¼šæ˜¯æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæœªè¢«æ˜¾ç¤ºè®­ç»ƒçš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚

![image.png](/attachment/Transformer/image.png)

### ***Working with text data***

ğŸthe required steps for preparing the embeddings used by an LLM

splitting text into individual word and subword tokens

converting words into tokens

turning tokens into embedding vectors

ğŸbe encoded into vector representations

ğŸadvanced tokenization schemes like byte pair encoding

ğŸimplement a sampling and data-loading strategy to produce the input-output pairs

*2.1 Understanding word embeddings*

embeddingï¼šå°†data convertæˆvector representionã€‚

an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector spaceã€‚

å› æ­¤éœ€è¦represent words as continuous-valued vectorsã€‚ 

![image.png](/attachment/Transformer/image%201.png)

text embedingåŒ…å«word embedingã€embeddings for sentences, paragraphs, or whole documentsã€‚

Sentence or paragraph embeddings are popular choices for *retrieval-augmented generationã€‚*

Word2Vecæ€æƒ³æ˜¯ç›¸ä¼¼ä¸Šä¸‹æ–‡é‡Œçš„å•è¯å…·æœ‰ç›¸ä¼¼çš„è¯­ä¹‰ï¼ŒæŠ•å½±åˆ°å‘é‡ç©ºé—´æ—¶clustered togetherã€‚

LLMä¼šè‡ªå·±ç”ŸæˆåµŒå…¥å‘é‡è€Œä¸æ˜¯ç”¨pretrained models such as Word2Vecã€‚

The smallest GPT-2 models (117M and 125M parameters)ï¼šan embedding size of 768 dimensionsã€‚

The largest GPT-3 model (175B parameters)ï¼šan embedding size of 12,288 dimensions

2.2 Tokenizing text

spliting input text into individual tokens

![image.png](/attachment/Transformer/image%202.png)

```python
# use the re,split command with the following syntax to split a text on whitespace charaters
import re
text = "Hello, world. This, is a test."
result = re.split(r'(\s)', text)
print(result)
```

The result is a list of individual words,whitespaces,and punctuation characters

`['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']`

```python
# modify the regular expression splits on whitespaces (\s), commas, and periods ([,.])
result = re.split(r'([,.]|\s)', text)
print(result)
```

the words and punctuation characters are now separate list entries

`['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is',' ', 'a', ' ', 'test', '.', '']`

```python
# remove these redundant characters
# strip() æ˜¯å­—ç¬¦ä¸²ï¼ˆstrï¼‰å¯¹è±¡çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨æ¥å»æ‰å­—ç¬¦ä¸²ä¸¤ç«¯çš„æŒ‡å®šå­—ç¬¦ï¼ˆé»˜è®¤æ˜¯ç©ºç™½ç¬¦ï¼ŒåŒ…æ‹¬ç©ºæ ¼ã€æ¢è¡Œç¬¦\nã€åˆ¶è¡¨ç¬¦\t ç­‰ï¼‰
result = [item for item in result if item.strip()]
print(result)
```

whitespace-free output

`['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']`

Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example,Python code, which is sensitive to indentation and spacing).

```python
# handle other types of punctuation, such as question marks, quotation marks, and the double-dashes
text = "Hello, world. Is this-- a test?"
result = re.split(r'([,.:;?_!"()\']|--|\s)', text)
result = [item.strip() for item in result if item.strip()]
print(result)
```

`['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']`

*2.3 Converting tokens into token IDs*

convert these tokens from a Python string to an integer representation to produce the token IDsã€‚

build a vocabularyã€‚This vocabulary defines how we map each unique word and special character to a unique integerã€‚

![image.png](/attachment/Transformer/image%203.png)

```python
# create a list of all unique tokens and sort them alphabetically
all_words = sorted(set(preprocessed))

# create the vocabulary which defines how we map each unique word and special character to a unique integer
vocab = {token:integer for integer,token in enumerate(all_words)}
```

apply this vocabulary to convert new text into token IDs and turn token IDs into textã€‚

![image.png](/attachment/Transformer/image%204.png)

```python
# implement a complete tokenizer class
# with an encode method that splits text into tokens 
# and carries out the string-to-integer mapping to produce tokenIDs via the vocabulary
# a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.
class SimpleTokenizerV1:
	def __init__(self, vocab):
		self.str_to_int = vocab 
		self.int_to_str = {i:s for s,i in vocab.items()} 
		# .items() return all key-value pairs of dict.such as dict_items([('hello', 1), ('world', 2)])

	def encode(self, text): 
		preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)
		preprocessed = [
		item.strip() for item in preprocessed if item.strip()]
		# ifåçš„stripæ˜¯åˆ¤æ–­æœ‰æ²¡æœ‰ç©ºå†…å®¹ï¼Œæ¯”å¦‚å…¨ç©ºæ ¼æˆ–è€…æ¢è¡Œç¬¦ï¼Œåˆ¤æ–­falseè¿‡æ»¤æ‰
		# æœ€å‰é¢çš„stripæ˜¯å¦‚æœä¸æ˜¯å…¨ç©ºå†…å®¹ï¼Œå°±å¯¹å†…å®¹è¿›è¡Œæ¸…æ´—å»æ‰ç©ºæ ¼æ¢è¡Œç¬¦ä¿ç•™æ¸¸æ³³å†…å®¹
		# æ²¡æœ‰ifåçš„stripå‰é¢çš„stripèƒ½ä¸èƒ½æ¸…æ´—æ‰å…¨ç©ºçš„å†…å®¹,ä¸è¡Œã€‚â€œ    â€.strip()ä¼šå˜æˆâ€œâ€
		ids = [self.str_to_int[s] for s in preprocessed]
		return ids

	def decode(self, ids): 
		text = " ".join([self.int_to_str[i] for i in ids]) 

		text = re.sub(r'\s+([,.?!"()\'])', r'\1', text) 
		# åœ¨å­—ç¬¦ä¸² string ä¸­ï¼ŒæŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆ pattern çš„å­ä¸²ï¼Œå¹¶ç”¨ repl æ›¿æ¢æ‰ã€‚
		# åœ¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ï¼ˆre.subï¼‰é‡Œï¼Œ\1 è¡¨ç¤ºï¼šå¼•ç”¨ç¬¬ 1 ä¸ªæ‹¬å·é‡Œæ•è·çš„å†…å®¹ã€‚
		# \s+ â†’ ä¸€ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ï¼‰
		# ([,.?!"()']) â†’ æ•è·æ‹¬å·å†…çš„ä»»æ„ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·
		# \1 â†’ æ­£åˆ™ä¸­ç¬¬ä¸€ä¸ªæ‹¬å·æ•è·çš„å†…å®¹ï¼ˆå³æ ‡ç‚¹ç¬¦å·æœ¬èº«ï¼‰
		# ä½œç”¨å°±æ˜¯ï¼šå»æ‰æ ‡ç‚¹å‰é¢çš„å¤šä½™ç©ºæ ¼ã€‚
		
		
		return text
		

"""
In [18]: a = ['Hello', 'world', ' ! ', '     ']

In [19]: [item.strip() for item in a]
Out[19]: ['Hello', 'world', '!', '']

In [20]: [item.strip() for item in a if item.strip()]
Out[20]: ['Hello', 'world', '!']

In [21]: [item for item in a if item.strip()]
Out[21]: ['Hello', 'world', ' ! ']

"""
```

Using the SimpleTokenizerV1 Python class, we can now instantiate new tokenizer objects via an existing vocabulary, which we can then use to encode and decode text

![image.png](/attachment/Transformer/image%205.png)

```python
tokenizer = SimpleTokenizerV1(vocab)
text = """"It's the last he painted, you know," 
 Mrs. Gisburn said with pardonable pride."""
ids = tokenizer.encode(text)
print(ids)

print(tokenizer.decode(ids))
```

*2.4 Adding special context tokens*

modify the tokenizer to handle unknown words and address the usage and addition of special context tokensã€‚

special tokens including markers for unknown words and document boundaries, <|unk|> and <|endoftext|>

![image.png](/attachment/Transformer/image%206.png)

```python
# add <unk> and <|endoftext|> to list of all unique words.

all_tokens = sorted(list(set(preprocessed)))
all_tokens.extend(["<|endoftext|>", "<|unk|>"])
vocab = {token:integer for integer,token in enumerate(all_tokens)}
```

A simple text tokenizer that handles unknown words

```python
class SimpleTokenizerV2:
	def __init__(self, vocab):
		self.str_to_int = vocab
		self.int_to_str = { i:s for s,i in vocab.items()}
	
	def encode(self, text):
		preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
		preprocessed = [
		item.strip() for item in preprocessed if item.strip()
		]
		preprocessed = [item if item in self.str_to_int 
		else "<|unk|>" for item in preprocessed]
		ids = [self.str_to_int[s] for s in preprocessed]
		return ids
	
	def decode(self, ids):
		text = " ".join([self.int_to_str[i] for i in ids])
		text = re.sub(r'\s+([,.:;?!"()\'])', r'\1', text) 
		return text
		
```

*2.5 Byte pair encoding*

Python open source library called *tiktoken* (https://github.com/openai/tiktoken), which implements the BPE algorithm very efficiently based on source code in Rust.

`pip install tiktoken`

The code we will use is based on tiktoken 0.7.0.

```python
from importlib.metadata import version
import tiktoken
print("tiktoken version:", version("tiktoken"))

# instantiate the BPE tokenizer from tiktoken
tokenizer = tiktoken.get_encoding("gpt2")

# an encode method:
text = (
 "Hello, do you like tea? <|endoftext|> In the sunlit terraces"
 "of someunknownPlace."
)
integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
print(integers)

# the decode method
strings = tokenizer.decode(integers)
print(strings)
```

the BPE tokenizer has a total vocabulary size of 50,257

BPE breaks down words that arenâ€™t in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words.

*2.6 Data sampling with a sliding window*

*2.7 Creating token embeddings*

convert the token IDs into embedding vectors

![image.png](/attachment/Transformer/image%207.png)

how the token ID to embedding vector conversion

```python
# the embedding layer is essentially a lookup operation 
# that retrieves rows from the embedding layerâ€™s weight matrix via a token ID.
torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weight)

# apply embedding layer to a token ID to obtain the embedding vector
print(embedding_layer(torch.tensor([3])))
print(embedding_layer(torch.tensor([2, 3, 5, 1])))

# Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix

```

*2.8 Encoding word positions*

it is helpful to inject additional position information into the LLM

two broad categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings

OpenAIâ€™s GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original transformer model.

```python
# token embedding
vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

# position embedding
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)
pos_embeddings = pos_embedding_layer(torch.arange(context_length))

# add pos_embeddings to token_embeddings
input_embeddings = token_embeddings + pos_embeddings
```

![image.png](/attachment/Transformer/image%208.png)

### *Coding attention mechanisms*

ğŸsimplified attention mechanism

ğŸadd a causal attention mask to prevent the LLM from accessing future tokens

ğŸadd a dropout mask to reduce overfitting in LLMs

ğŸmulti-head attention: multiple instances of causal attention

ğŸcreating multi-head attention modules involves batched matrix multiplications

*3.4.2 Implementing a compact self-attention Python class*

1. initializes trainable weight matrices (W_query, W_key, and W_value)
2. compute the attention scores (attn_scores) by multiplying queries and keys
3. normalizing these scores using softmax to get attn_weights
4. create a context vector by weighting the values with these attn_weights

$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V$

a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.

```python

class SelfAttention_v2(nn.Module):
	def __init__(self, d_in, d_out, qkv_bias=False):
		super().__init__()
		
		self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
	def forward(self, x): # shape of x: (seq_len, d_in)
		keys = self.W_key(x) # shape of keys: (seq_len, d_out)
		queries = self.W_query(x) # shape of keys: (seq_len, d_out)
		values = self.W_value(x) # shape of keys: (seq_len, d_out)
		attn_scores = queries @ keys.T # (seq_len, seq_len)
		attn_weights = torch.softmax(
		attn_scores / keys.shape[-1]**0.5, dim=-1
		) # keys.shape[-1]: d_out
		# torch.softmax(......, dim=-1),æŒ‰æœ€åä¸€ç»´ç®—softmaxï¼Œä¹Ÿå°±æ˜¯æŒ‰è¡Œ
		context_vec = attn_weights @ values
		return context_vec
		
	
# inputs contains six embedding vectors
# results contains six context vectors	
torch.manual_seed(123)
sa_v1 = SelfAttention_v1(d_in, d_out)
print(sa_v1(inputs))
```

![image.png](/attachment/Transformer/image%209.png)

*3.5 Hiding future words with causal attention*

Causal attention, also known as masked attention è®©æ¨¡å‹åªå…³æ³¨sequenceä¸­çš„previous å’Œ current input

mask out the future tokens

![image.png](/attachment/Transformer/image%2010.png)

dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors.

apply the dropout mask after computing the attention weightsæ›´å¸¸è§ã€‚

![image.png](/attachment/Transformer/image%2011.png)

```python
class CausalAttention(nn.Module):
	def __init__(self, d_in, d_out, context_length,
		dropout, qkv_bias=False):
		super().__init__()
		self.d_out = d_out
		self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.dropout = nn.Dropout(dropout) 
		self.register_buffer(
		'mask',
		torch.triu(torch.ones(context_length, context_length),
		diagonal=1)
		) 
	def forward(self, x):
		b, num_tokens, d_in = x.shape 
		
		keys = self.W_key(x)
		queries = self.W_query(x)
		values = self.W_value(x)
		
		attn_scores = queries @ keys.transpose(1, 2) 
		# creating a mask with 1s above the diagonal 
		# and then replacing these 1s with negative infinity (-inf) values
		attn_scores.masked_fill_( 
		self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) 
		
		attn_weights = torch.softmax(
		attn_scores / keys.shape[-1]**0.5, dim=-1
		)
		
		attn_weights = self.dropout(attn_weights)
		context_vec = attn_weights @ values
		return context_vec
		
		
		
torch.manual_seed(123)
context_length = batch.shape[1]
ca = CausalAttention(d_in, d_out, context_length, 0.0)
context_vecs = ca(batch)
print("context_vecs.shape:", context_vecs.shape)
```

*3.6 Extending single-head attention to multi-head attention*

processing the heads in sequential

multiple heads are implemented by creating a list of CausalAttention objects (self.heads)

![image.png](/attachment/Transformer/image%2012.png)

```python
class MultiHeadAttentionWrapper(nn.Module):
	def __init__(self, d_in, d_out, context_length,
		dropout, num_heads, qkv_bias=False):
		super().__init__()
		self.heads = nn.ModuleList(
		[CausalAttention(
		d_in, d_out, context_length, dropout, qkv_bias
		) 
		for _ in range(num_heads)]
		)
	def forward(self, x):
		return torch.cat([head(x) for head in self.heads], dim=-1)
```

processing the heads in parallel

splits the input into multiple heads by reshaping the projected query, key, and value
tensors and then combines the results from these heads after computing attention

split the d_out dimension into num_heads and head_dim, where head_dim = d_out / num_heads.

This splitting is then achieved using the .view method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens, num_heads, head_dim)

![image.png](/attachment/Transformer/image%2013.png)

```python
class MultiHeadAttention(nn.Module):
	def __init__(self, d_in, d_out, 
		context_length, dropout, num_heads, qkv_bias=False):
		super().__init__()
		assert (d_out % num_heads == 0), \
		"d_out must be divisible by num_heads"
		self.d_out = d_out
		self.num_heads = num_heads
		self.head_dim = d_out // num_heads 
		self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
		self.out_proj = nn.Linear(d_out, d_out) 
		self.dropout = nn.Dropout(dropout)
		self.register_buffer(
		"mask",
		torch.triu(torch.ones(context_length, context_length),
		diagonal=1)
		)
	def forward(self, x):
		b, num_tokens, d_in = x.shape
		keys = self.W_key(x) 
		queries = self.W_query(x) 
		values = self.W_value(x) 
		keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) 
		values = values.view(b, num_tokens, self.num_heads, self.head_dim) 
		queries = queries.view( 
		b, num_tokens, self.num_heads, self.head_dim 
		) 
		keys = keys.transpose(1, 2) 
		queries = queries.transpose(1, 2) 
		values = values.transpose(1, 2) 
		attn_scores = queries @ keys.transpose(2, 3) 
		mask_bool = self.mask.bool()[:num_tokens, :num_tokens] 
		
		attn_scores.masked_fill_(mask_bool, -torch.inf) 
		attn_weights = torch.softmax(
		attn_scores / keys.shape[-1]**0.5, dim=-1)
		attn_weights = self.dropout(attn_weights)
		context_vec = (attn_weights @ values).transpose(1, 2) 
		
		context_vec = context_vec.contiguous().view(
		b, num_tokens, self.d_out
		)
		context_vec = self.out_proj(context_vec) 
		return context_vec

		
torch.manual_seed(123)
batch_size, context_length, d_in = batch.shape
d_out = 2
mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)
context_vecs = mha(batch)
print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
```

### *Implementing a GPT model from scratch to generate text*

*This chapter covers*

- Coding a GPT-like large language model (LLM) that can be trained to generate human-like text
- Normalizing layer activations to stabilize neural network training
- Adding shortcut connections in deep neural networks
- Implementing transformer blocks to create GPT models of various sizes
- Computing the number of parameters and storage requirements of GPT models

*4.1 Coding an LLM architecture*

parametersæŒ‡çš„æ˜¯trainable weights of the model

a GPT placeholder architecture (DummyGPTModel)

the order in which we tackle the individual concepts required to code the final GPT architecture

å…ˆcodeå‡ºä¸€ä¸ªGPT placeholder architecture calling DummyGPTModelï¼Œç„¶åå¾—åˆ°the individual core pieces ï¼Œæœ€ç»ˆassemblingèµ·æ¥ã€‚

ä¸€ä¸ªDummyGPTModelåŒ…æ‹¬token embeddings , positional embedding , dropout , ä¸€ç³»åˆ—çš„transformer blocks(DummyTransformerBlock) æœ€åä¸€ä¸ªLayer Normalization(DummyLayerNorm)å’Œä¸€ä¸ªLinear output layer

![image.png](/attachment/Transformer/image%2014.png)

```python
GPT_CONFIG_124M = {
	"vocab_size": 50257, # Vocabulary size
	"context_length": 1024, # modelèƒ½å¤Ÿå¤„ç†çš„æœ€å¤§tokenå’Œpositional embeddingæ•°é‡
	"emb_dim": 768, # æ¯ä¸ªTokençš„Embedding size
	"n_heads": 12, # Number of attention heads
	"n_layers": 12, # Transformer Blockçš„æ•°é‡
	"drop_rate": 0.1, # Dropout rate
	"qkv_bias": False # Query-Key-Value bias
}
# A placeholder GPT model architecture class
import torch
import torch.nn as nn
	class DummyGPTModel(nn.Module):
	def __init__(self, cfg):
		super().__init__()
		self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
		self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
		self.drop_emb = nn.Dropout(cfg["drop_rate"])
		self.trf_blocks = nn.Sequential( 
		*[DummyTransformerBlock(cfg) 
		for _ in range(cfg["n_layers"])] 
		) 
		# *èµ·unpackçš„ä½œç”¨ï¼Œå…·ä½“æ˜¯å°†åˆ—è¡¨é‡Œçš„å…ƒç´ ç»™ä¸€ä¸€æ‹¿å‡ºæ¥ã€‚
		self.final_norm = DummyLayerNorm(cfg["emb_dim"]) 
		self.out_head = nn.Linear(
		cfg["emb_dim"], cfg["vocab_size"], bias=False
		)
	def forward(self, in_idx):
		batch_size, seq_len = in_idx.shape
		tok_embeds = self.tok_emb(in_idx)
		pos_embeds = self.pos_emb(
		torch.arange(seq_len, device=in_idx.device)
		)
		x = tok_embeds + pos_embeds
		x = self.drop_emb(x)
		x = self.trf_blocks(x)
		x = self.final_norm(x)
		logits = self.out_head(x)
		return logits
		
class DummyTransformerBlock(nn.Module): 
	def __init__(self, cfg):
		super().__init__()
	def forward(self, x): 
		return x
class DummyLayerNorm(nn.Module): 
	def __init__(self, normalized_shape, eps=1e-5): 
		super().__init__()
	def forward(self, x):
		return x
```

*4.2 Normalizing activations with layer normalization*

ä¸ºä»€ä¹ˆç”¨batch normalizationè€Œä¸æ˜¯layer normalizationï¼Ÿ

è®­ç»ƒæœ‰è®¸å¤šlayersçš„deep neural networkæ—¶ä¼šé‡åˆ°vanishing or exploding gradientsçš„é—®é¢˜ã€‚

å®ç°Layer normalizationå¯ä»¥æå‡è®­ç»ƒçš„stabilityå’Œefficiencyï¼Œ

layer normalizationä¸€èˆ¬æ˜¯æ”¾åœ¨multi-head attention moduleçš„å‰å

dim=-1è¡¨ç¤ºå‘é‡çš„æœ€åä¸€ç»´ï¼Œå¯¹äºä¸€ä¸ªtwo dimensional tensoræ¥è¯´ä¹Ÿå°±æ˜¯å‘é‡çš„columnsã€‚å¯¹äºä¸€ä¸ªä¸‰ç»´å‘é‡[bs, seq_len, embedding_size]æ¥è¯´æœ€åä¸€ç»´å°±æ˜¯æ¯ä¸ªtokençš„embeddin

_size.

layer normalizationçš„æ“ä½œï¼šout_norm = (out - mean) / torch.sqrt(var).

layer normalizationä¸€èˆ¬æ˜¯åœ¨è¾“å…¥tensorçš„last dimensionæ“ä½œçš„ï¼Œè¿™ä»£è¡¨embedding dimension (emb_dim)ã€‚

```python
	class LayerNorm(nn.Module):
	def __init__(self, emb_dim):
		super().__init__()
		self.eps = 1e-5
		self.scale = nn.Parameter(torch.ones(emb_dim))
		self.shift = nn.Parameter(torch.zeros(emb_dim))
	def forward(self, x):
		mean = x.mean(dim=-1, keepdim=True)
		# é‡‡ç”¨Biased varianceæœ‰åä¼°è®¡æ¥æ±‚æ ·æœ¬æ–¹å·®ï¼Œå› ä¸ºç»´åº¦768è¶³å¤Ÿå¤§ï¼ŒåŒºåˆ«ä¸å¤§
		var = x.var(dim=-1, keepdim=True, unbiased=False)
		norm_x = (x - mean) / torch.sqrt(var + self.eps)
		# epsæ˜¯æå°å€¼ï¼Œé˜²æ­¢division by zeroçš„æƒ…å†µ
		return self.scale * norm_x + self.shift
		# scale å’Œ shiftæ˜¯ä¸¤ä¸ªtrainable parameters,åˆå§‹åˆ†åˆ«æ˜¯1ï¼Œ0ï¼Œ
		# è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚æœè¢«åˆ¤å®šè°ƒæ•´ä»–èƒ½æå‡performanceï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨çš„è°ƒæ•´ä»–ã€‚
```

æ–¹å·®ä¸æ ·æœ¬æ–¹å·®ï¼Œä»¥åŠæ ·æœ¬æ–¹å·®çš„æœ‰åä¼°è®¡ä¸æ— åä¼°è®¡ã€‚

æ–¹å·®æ˜¯é™¤nï¼Œä¸ºä»€ä¹ˆæ ·æœ¬æ–¹å·®é™¤nåè€Œæ˜¯æœ‰åä¼°è®¡ï¼Œæ ·æœ¬æ–¹å·®æ— åä¼°è®¡æ˜¯é™¤ä»¥n-1ï¼Ÿå› ä¸ºé‡‡æ ·è¿‡ç¨‹ä¸­ä¼šä½ä¼°æ–¹å·®ï¼Œæ‰€ä»¥è¦é€šè¿‡è´å¡å°”ä¿®æ­£æ¥ä¿®æ­£æ–¹å·®ã€‚æ–¹å·®æ˜¯ååº”æ•°æ®ç¦»æ•£ç¨‹åº¦çš„ï¼Œä»æ‰€æœ‰çš„æ•°æ®ä¸­é‡‡æ ·ä¸€äº›æ ·æœ¬ï¼Œç”¨æ ·æœ¬çš„æ ·æœ¬æ–¹å·®æ¥ååº”æ€»ä½“æ–¹å·®çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯é‡‡æ ·æ ·æœ¬çš„è¿‡ç¨‹è‚¯å®šæ˜¯æ¦‚ç‡è¶Šå¤§çš„è¶Šå®¹æ˜“é‡‡æ ·ï¼Œå°±å¯¼è‡´é‡‡æ ·çš„æ ·æœ¬ç¦»æ•£ç¨‹åº¦æ›´é›†ä¸­ï¼Œå½“ç„¶å¦‚æœé‡‡æ ·çš„æ•°ç›®nè¶³å¤Ÿå¤§ï¼Œé‡‡æ ·çš„æ ·æœ¬çš„åˆ†å¸ƒå°±æ— é™åˆ†å¸ƒæ€»ä½“çš„åˆ†å¸ƒäº†ã€‚ä»¥å¦‚ä¸‹æ­£å¤ªåˆ†å¸ƒä¸ºä¾‹ã€‚

![image.png](/attachment/Transformer/image%2015.png)

*4.3 Implementing a feed forward network with GELU activations*

ä¸ºä»€ä¹ˆç”¨GELUï¼ˆé¸¡è·¯ï¼‰æ¿€æ´»å‡½æ•°è€Œä¸æ˜¯reluæ¿€æ´»å‡½æ•°ï¼Ÿå› ä¸ºreluæ¿€æ´»å‡½æ•°åœ¨0çš„åœ°æ–¹ä¸å¯å¾®ã€‚

dead neuronsï¼šå½“è¾“å…¥å°äº0æ—¶è¾“å‡ºæ°¸è¿œæ˜¯0å¯¹å­¦ä¹ æ²¡æœ‰ä»€ä¹ˆè´¡çŒ®ï¼Œæ‰€ä»¥å«dead neuronsã€‚

GELU(x) = xâ‹…Î¦(x), where Î¦(x) is the cumulative distribution function of the standard Gaussian distributionã€‚

standard Gaussian distribution

![image.png](/attachment/Transformer/image%2016.png)

standard Gaussian distributionçš„CDF

![image.png](/attachment/Transformer/image%2017.png)

(the original GPT-2 model was also trained with this approximation, which was found via curve fittingï¼‰

![image.png](/attachment/Transformer/image%2018.png)

m = nn.GELU()

An implementation of the GELU activation function

```python
class GELU(nn.Module):
	def __init__(self):
		super().__init__()
	def forward(self, x):
		return 0.5 * x * (1 + torch.tanh(
		torch.sqrt(torch.tensor(2.0 / torch.pi)) * 
		(x + 0.044715 * torch.pow(x, 3))
		))
```

A feed forward neural network module

```python
class FeedForward(nn.Module):
	def __init__(self, cfg):
		super().__init__()
		self.layers = nn.Sequential(
		nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
		GELU(),
		nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
		)
	def forward(self, x):
		return self.layers(x)
```

![image.png](/attachment/Transformer/image%2019.png)

*4.4 Adding shortcut connections*

short connectionså’Œresidual connectionæ˜¯ä¸€å›äº‹ï¼Œç”¨æ¥è§£å†³vanishing gredient problemã€‚

*vanishing gradient problem* æ˜¯æŒ‡åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦è¿‡å°å¯¼è‡´å­¦ä¹ åœæ»ä¸å‰ï¼Œconvergence delayã€‚

ä¸åŠ Skip Connection åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦ä¼šè¶Šæ¥è¶Šå°ã€‚skip connectionå¯ä»¥ä¸ºæ¢¯åº¦åˆ›å»ºä¸€ä¸ªå¦å¤–çš„æ›´çŸ­çš„è·¯å¾„ï¼Œé€šè¿‡skipping one or more layersè®©gradient flow

![image.png](/attachment/Transformer/image%2020.png)

![Visualizing the Loss Landscape of Neural Nets](/attachment/Transformer/image%2021.png)

Visualizing the Loss Landscape of Neural Nets

ä¸åŠ Skip Connectionæœ‰å¾ˆå¤šçš„å±€éƒ¨æœ€å°å€¼ï¼Œä¼šå¯¼è‡´ä¼˜åŒ–å›°éš¾

```python
class ExampleDeepNeuralNetwork(nn.Module):
	def __init__(self, layer_sizes, use_shortcut):
		super().__init__()
		self.use_shortcut = use_shortcut
		self.layers = nn.ModuleList([
		nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), 
		GELU()),
		nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), 
		GELU()),
		nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), 
		GELU()),
		nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), 
		GELU()),
		nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), 
		GELU())
		])
	def forward(self, x):
		for layer in self.layers:
			layer_output = layer(x) 
			if self.use_shortcut and x.shape == layer_output.shape: 
				x = x + layer_output
			else:
				x = layer_output
		return x
```

a function that computes the gradients

```python
def print_gradients(model, x):
	output = model(x) 
	target = torch.tensor([[0.]])
	loss = nn.MSELoss()
	loss = loss(output, target) 
	loss.backward()
	for name, param in model.named_parameters():
		if 'weight' in name:
		print(f"{name} has gradient mean of {param.grad.abs().mean().item()}") 
```

*4.5 Connecting attention and linear layers in a transformer block*

implement the *transformer block åŒ…æ‹¬* multi-head attention, layer normalization, dropout, feed forward layers, and GELU activationsã€‚

TransformerBlockçš„æ ¸å¿ƒæ˜¯åŒ…æ‹¬ a multi-head attention mechanism (MultiHeadAttention) and a feed forward network (FeedForward)ã€‚å…¶ä¸­Layer normalization (LayerNorm)æ˜¯åœ¨ä»¥ä¸Šä¸¤éƒ¨åˆ†çš„å‰é¢ï¼Œdropoutæ˜¯åœ¨ä»¥ä¸Šä¸¤éƒ¨åˆ†çš„åé¢ã€‚layer normalizationåœ¨å‰é¢æ˜¯å«Pre-LayerNormï¼Œåœ¨åŸè®ºæ–‡ä¸­LayerNormæ˜¯åœ¨åé¢å«Post-LayerNorm

![image.png](/attachment/Transformer/image%2022.png)

```python
class TransformerBlock(nn.Module):
	def __init__(self, cfg):
		super().__init__()
		
		self.att = MultiHeadAttention(
			d_in=cfg["emb_dim"],
			d_out=cfg["emb_dim"],
			context_length=cfg["context_length"],
			num_heads=cfg["n_heads"], 
			dropout=cfg["drop_rate"],
			qkv_bias=cfg["qkv_bias"])
		self.ff = FeedForward(cfg)
		self.norm1 = LayerNorm(cfg["emb_dim"])
		self.norm2 = LayerNorm(cfg["emb_dim"])
		self.drop_shortcut = nn.Dropout(cfg["drop_rate"])
		
	def forward(self, x):
	
		shortcut = x
		x = self.norm1(x)
		x = self.att(x)
		x = self.drop_shortcut(x)
		x = x + shortcut 
		shortcut = x 
		x = self.norm2(x)
		x = self.ff(x)
		x = self.drop_shortcut(x)
		x = x + shortcut 
		return x
```

*4.6 Coding the GPT model*

![image.png](/attachment/Transformer/image%2023.png)

```python
class GPTModel(nn.Module):
	def __init__(self, cfg):
		super().__init__()
		self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
		self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
		self.drop_emb = nn.Dropout(cfg["drop_rate"])
		
		self.trf_blocks = nn.Sequential(
		*[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])
		
		self.final_norm = LayerNorm(cfg["emb_dim"])
		self.out_head = nn.Linear(
		cfg["emb_dim"], cfg["vocab_size"], bias=False
		)
	def forward(self, in_idx):
		batch_size, seq_len = in_idx.shape
		tok_embeds = self.tok_emb(in_idx)
		
		pos_embeds = self.pos_emb(
		torch.arange(seq_len, device=in_idx.device)
		)
		x = tok_embeds + pos_embeds
		x = self.drop_emb(x)
		x = self.trf_blocks(x)
		x = self.final_norm(x)
		logits = self.out_head(x)
		return logits
```

```python
# è®¡ç®—modelçš„parametersæ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"Total number of parameters: {total_params:,}")

# è®¡ç®—modelçš„parameterså çš„å†…å­˜å¤§å°
# Calculates the total size in bytes (assuming float32, 4 bytes per parameter)
total_size_bytes = total_params * 4 
# Converts to megabytes
total_size_mb = total_size_bytes / (1024 * 1024) 
print(f"Total size of the model: {total_size_mb:.2f} MB")
```

ä»¥ä¸Šæ–¹å¼è®¡ç®—çš„æ€»å‚æ•°é‡æ˜¯163,009,536ï¼Œå®é™…GPT-2çš„å‚æ•°é‡æ˜¯124,412,160ã€‚åŸå› æ˜¯GPT-2é‡Œæœ‰ä¸ªå‚æ•°ç»‘å®šï¼ˆweight tyingï¼‰çš„æŠ€æœ¯ã€‚å…·ä½“åšæ³•å°±æ˜¯åœ¨output layeré‡Œå¤ç”¨äº†the token embedding layerçš„æƒé‡ã€‚ä¸ºä»€ä¹ˆè¦è¿™æ ·åšäº†ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªå±‚çš„ç»´åº¦æ˜¯vocabulary size50, 257éå¸¸çš„å·¨å¤§ã€‚

*4.7 Generating text*

*greedy decoding: å–æ¦‚ç‡æœ€å¤§çš„ä½ç½®å¤„çš„tokenã€‚*

ç”¨softmax functionå»å°†logitsè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œç”¨torch.argmaxé€‰å‡ºæ¦‚ç‡æœ€å¤§å¤„çš„ç´¢å¼•ã€‚

![image.png](/attachment/Transformer/image%2024.png)

```python
def generate_text_simple(model, idx, max_new_tokens, context_size): 
	'''
		max_new_tokens:å¸Œæœ›ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡ã€‚
		context_size:æ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
	'''
	# å¾ªç¯max_new_tokensæ¬¡ï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ–°çš„token
	 for _ in range(max_new_tokens):
		 # å—é™äºæ¨¡å‹èƒ½å¤„ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦context_size,é€‰æœ€æ–°çš„context_sizeä¸ªtokenã€‚
		 idx_cond = idx[:, -context_size:] 
		 with torch.no_grad():
			 logits = model(idx_cond)
		 # é€‰å‡ºæ¨¡å‹generateçš„token: (bs, n_tokens, vocab_size) ---> (bs, vocab_size)
		 logits = logits[:, -1, :] 
		 probas = torch.softmax(logits, dim=-1) # logits ---> Probability distribution
		 idx_next = torch.argmax(probas, dim=-1, keepdim=True) # (bs,1)
		 idx = torch.cat((idx, idx_next), dim=1) 
```

### *Pretraining on unlabeled data*

*5.1.1 Using GPT to generate text*

```python
import tiktoken
from chapter04 import generate_text_simple

def text_to_token_ids(text, tokenizer):
	encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
	encoded_tensor = torch.tensor(encoded).unsqueeze(0) 
	# .unsqueeze(0) adds the batch dimension
	return encoded_tensor
def token_ids_to_text(token_ids, tokenizer):
	flat = token_ids.squeeze(0) 
	# Removes batch dimension 
	return tokenizer.decode(flat.tolist())
	
start_context = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")
token_ids = generate_text_simple(
	model=model,
	idx=text_to_token_ids(start_context, tokenizer),
	max_new_tokens=10,
	context_size=GPT_CONFIG_124M["context_length"]
	)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

*5.1.2 Calculating the text generation loss*

not just generating next token but also measuring the quality of the generated token