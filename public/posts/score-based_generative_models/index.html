<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Score Based Generative Models | tyh's blog</title><meta name=keywords content><meta name=description content="https://www.youtube.com/watch?v=wMmqCMwuM2Q
https://www.youtube.com/watch?v=B4oHJpEJBAA
https://scorebasedgenerativemodeling.github.io/
https://yang-song.net/blog/2021/score/

使用“基于得分的生成模型”这个术语，是希望强调它们与得分函数之间的关系。
得分函数被定义为对数概率密度的梯度。
统计学和机器学习中的典型假设是，训练数据中的所有数据点都来自某种潜在的数据分布。换句话说，这些数据点基本上是从该数据分布中独立抽取的样本，但我们并没有数据分布的解析形式，我们需要对其进行估计。
为了估计这个数据分布，我们需要创建一个模型。这个模型表示了一个参数化的概率分布，我们称之为模型分布。我们希望调整这个模型的参数，以确保它在某种意义上接近数据分布。
如果模型分布非常接近数据分布，那么我们可以将这个模型用于许多重要的应用。例如，当然，我们可以通过从这个模型分布中采样来生成无限数量的新的数据点。
另一个应用是，我们可以使用这个模型分布来计算任何潜在数据点的概率值。举个例子，对于一个数据点，比如一张吉娃娃的照片，因为它是一张狗的照片，所以它实际上属于我们的数据分布。因此，这个模型分布通常会为这样的数据点分配较高的概率值。对于一些不相关的数据点，比如一张松饼的照片，因为它不是狗的照片，一个好的模型分布会为这种图片分配较低的概率值。因为这个模型分布提供了一种生成新的数据点的方式，所以我们也将其称为生成模型。
模型提供了一组概率分布族。我们希望通过最小化 $𝑃_𝜃$与 $𝑃_{data}$ 之间的距离，从这个庞大的分布族中找到一个特定的概率分布。之后，我们就可以从 $𝑃_{data}$ 中生成样本。
数据分布可能非常复杂，尤其是当数据维度很高时。因此，我们必须构建一个非常强大的模型分布，才能估计我们的数据分布。那么，如何构建一个强大的模型分布呢？
得分函数：得分函数用于表示我们的概率分布。那么，什么是得分函数呢？假设我们有一个连续的概率分布，使用 𝑝(𝑥) 来表示概率密度函数。我们将得分函数定义为对 log⁡𝑝(𝑥) 的梯度。注意这个梯度是相对于随机变量 𝑥 计算的——它不是相对于任何模型参数（如 𝜃）计算的。
score function:从直觉上来说，分数函数也非常有意义：它告诉你，为了让某个数据点的概率变高，你应该往哪个方向移动它。真实的分数函数可以告诉你，在数据空间中的任意点，你应当往哪个方向移动，才能更接近真实数据点。
得分函数长什么样子呢？我们来看一个简单的例子，即两个高斯分布的混合。这个图展示了这个高斯混合分布的密度函数和得分函数。密度函数用颜色编码，颜色越深表示密度越高。得分函数是一个向量场，给出了密度函数增长最快的方向。给定密度函数，我们可以非常容易地计算得分函数，因为我们只需要对其进行求导。相反，利用得分函数，我们也可以通过计算积分来恢复密度函数。因此它们在理论上是等价的。但是在计算上，与密度函数相比，得分函数要更容易处理。

得分模型：根据我们在统计学中的知识，我们知道可以训练一个适当归一化的统计模型，使用如最大似然估计等方法来估计潜在的数据密度。但因为我们想使用得分函数，我们希望开发一个类似的方法，使我们能够训练我们的得分模型来估计潜在的得分函数，且只需一个有限的训练数据集。
从数学角度来看，我们给定一组数据点，假设这些数据点是从数据分布 $P_{\text{data}}​$ 中独立采样的。我们的目标是估计数据密度的得分函数。我们给定一个得分模型，这个模型假设是一个深度神经网络模型，能够将高维输入映射到高维输出，并且我们希望训练这个得分模型，使其近似于数据分布的真实得分函数。那么，如何训练这个得分模型，使其接近我们的真实数据得分函数呢？我们需要最小化某个目标函数。这个目标函数必须比较得分函数的两个向量场。这里，一个向量场是数据得分函数的真实值，另一个向量场是由我们的得分模型预测的。我们如何比较它们之间的差异呢？我们回顾一下，这两个向量场实际上位于相同的空间中。因此，我们可能能够计算这些向量对之间的差异向量，然后我们可以对这些差异向量的密度进行积分，形成一个标量值的目标函数。

Fisher divergence:
score matching:指的是用我们预测的分数 $𝑠_𝜃(𝑥)$去逼近原始分布的真实分数。这基本上意味着我们试图最小化真实分数和预测分数之间的差异
雅可比矩阵:
切片得分匹配（sliced score matching）: 基本的直觉是，一维问题应该比高维问题更容易解决。那么，我们如何将高维问题转换为一维问题呢？我们可以利用随机投影。我们将高维向量场投影到随机方向上。然后我们得到一维标量场。假设这两个高维向量场相互接近。那么我们可以沿着随机的一维方向投影它们。这样，我们就得到了接近的标量场。
denoising score matching：模型在训练时需要从带噪输入中预测负噪声。而预测负噪声的方向恰好是 朝向真实数据分布的方向——这正是“分数”的含义：告诉你朝哪里走概率更大。

给定一个大的训练数据集，我们可以使用基于原则的统计方法，如得分匹配，来训练我们的得分模型，以估计潜在的得分函数。为了构建我们的生成模型，我们必须找到一种方法，通过得分函数的向量场来生成新的数据点。假设我们已经给定了得分函数。想象一下有许多随机点散布在楼梯上。我们能否通过移动这些随机点来从得分函数中形成样本呢？可以通过跟随得分函数预测的方向来移动这些点。然而，这并不能给我们有效的样本，因为这些点最终会聚集在一起。将高斯噪声注入到得分函数中，并跟随这些噪声扰动的得分函数。

得分匹配可以估计数据的得分函数，Langevin 动力学可以从得分函数中生成样本。所以，用我们的得分模型替换 Langevin 动力学中的得分函数就变得非常自然了，然后我们就可以生成数据样本——从而定义一个新的生成模型。
Langevin 动力学：
score matching can estimate the score function data. And Langevin dynamics can generate samples from the score function. So it becomes very, very natural to just replace the score
function in Langevin dynamics with our score model, and then we can generate data samples&ndash;we define a new generative model."><meta name=author content="Me"><link rel=canonical href=https://tyh382596868.github.io/posts/score-based_generative_models/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.a5781257de4197b8e3d54346acdf1dd55a9ba4cb91dcace192fc1baf228c08b5.css integrity="sha256-pXgSV95Bl7jj1UNGrN8d1VqbpMuR3KzhkvwbryKMCLU=" rel="preload stylesheet" as=style><link rel=icon href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tyh382596868.github.io/posts/score-based_generative_models/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){if(typeof renderMathInElement!="function")return;renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredTags:["script","noscript","style","textarea","pre","code"],throwOnError:!1})})</script><meta property="og:url" content="https://tyh382596868.github.io/posts/score-based_generative_models/"><meta property="og:site_name" content="tyh's blog"><meta property="og:title" content="Score Based Generative Models"><meta property="og:description" content="https://www.youtube.com/watch?v=wMmqCMwuM2Q
https://www.youtube.com/watch?v=B4oHJpEJBAA
https://scorebasedgenerativemodeling.github.io/
https://yang-song.net/blog/2021/score/
使用“基于得分的生成模型”这个术语，是希望强调它们与得分函数之间的关系。
得分函数被定义为对数概率密度的梯度。
统计学和机器学习中的典型假设是，训练数据中的所有数据点都来自某种潜在的数据分布。换句话说，这些数据点基本上是从该数据分布中独立抽取的样本，但我们并没有数据分布的解析形式，我们需要对其进行估计。
为了估计这个数据分布，我们需要创建一个模型。这个模型表示了一个参数化的概率分布，我们称之为模型分布。我们希望调整这个模型的参数，以确保它在某种意义上接近数据分布。
如果模型分布非常接近数据分布，那么我们可以将这个模型用于许多重要的应用。例如，当然，我们可以通过从这个模型分布中采样来生成无限数量的新的数据点。
另一个应用是，我们可以使用这个模型分布来计算任何潜在数据点的概率值。举个例子，对于一个数据点，比如一张吉娃娃的照片，因为它是一张狗的照片，所以它实际上属于我们的数据分布。因此，这个模型分布通常会为这样的数据点分配较高的概率值。对于一些不相关的数据点，比如一张松饼的照片，因为它不是狗的照片，一个好的模型分布会为这种图片分配较低的概率值。因为这个模型分布提供了一种生成新的数据点的方式，所以我们也将其称为生成模型。
模型提供了一组概率分布族。我们希望通过最小化 $𝑃_𝜃$与 $𝑃_{data}$ 之间的距离，从这个庞大的分布族中找到一个特定的概率分布。之后，我们就可以从 $𝑃_{data}$ 中生成样本。
数据分布可能非常复杂，尤其是当数据维度很高时。因此，我们必须构建一个非常强大的模型分布，才能估计我们的数据分布。那么，如何构建一个强大的模型分布呢？
得分函数：得分函数用于表示我们的概率分布。那么，什么是得分函数呢？假设我们有一个连续的概率分布，使用 𝑝(𝑥) 来表示概率密度函数。我们将得分函数定义为对 log⁡𝑝(𝑥) 的梯度。注意这个梯度是相对于随机变量 𝑥 计算的——它不是相对于任何模型参数（如 𝜃）计算的。
score function:从直觉上来说，分数函数也非常有意义：它告诉你，为了让某个数据点的概率变高，你应该往哪个方向移动它。真实的分数函数可以告诉你，在数据空间中的任意点，你应当往哪个方向移动，才能更接近真实数据点。
得分函数长什么样子呢？我们来看一个简单的例子，即两个高斯分布的混合。这个图展示了这个高斯混合分布的密度函数和得分函数。密度函数用颜色编码，颜色越深表示密度越高。得分函数是一个向量场，给出了密度函数增长最快的方向。给定密度函数，我们可以非常容易地计算得分函数，因为我们只需要对其进行求导。相反，利用得分函数，我们也可以通过计算积分来恢复密度函数。因此它们在理论上是等价的。但是在计算上，与密度函数相比，得分函数要更容易处理。 得分模型：根据我们在统计学中的知识，我们知道可以训练一个适当归一化的统计模型，使用如最大似然估计等方法来估计潜在的数据密度。但因为我们想使用得分函数，我们希望开发一个类似的方法，使我们能够训练我们的得分模型来估计潜在的得分函数，且只需一个有限的训练数据集。
从数学角度来看，我们给定一组数据点，假设这些数据点是从数据分布 $P_{\text{data}}​$ 中独立采样的。我们的目标是估计数据密度的得分函数。我们给定一个得分模型，这个模型假设是一个深度神经网络模型，能够将高维输入映射到高维输出，并且我们希望训练这个得分模型，使其近似于数据分布的真实得分函数。那么，如何训练这个得分模型，使其接近我们的真实数据得分函数呢？我们需要最小化某个目标函数。这个目标函数必须比较得分函数的两个向量场。这里，一个向量场是数据得分函数的真实值，另一个向量场是由我们的得分模型预测的。我们如何比较它们之间的差异呢？我们回顾一下，这两个向量场实际上位于相同的空间中。因此，我们可能能够计算这些向量对之间的差异向量，然后我们可以对这些差异向量的密度进行积分，形成一个标量值的目标函数。 Fisher divergence:
score matching:指的是用我们预测的分数 $𝑠_𝜃(𝑥)$去逼近原始分布的真实分数。这基本上意味着我们试图最小化真实分数和预测分数之间的差异
雅可比矩阵:
切片得分匹配（sliced score matching）: 基本的直觉是，一维问题应该比高维问题更容易解决。那么，我们如何将高维问题转换为一维问题呢？我们可以利用随机投影。我们将高维向量场投影到随机方向上。然后我们得到一维标量场。假设这两个高维向量场相互接近。那么我们可以沿着随机的一维方向投影它们。这样，我们就得到了接近的标量场。
denoising score matching：模型在训练时需要从带噪输入中预测负噪声。而预测负噪声的方向恰好是 朝向真实数据分布的方向——这正是“分数”的含义：告诉你朝哪里走概率更大。 给定一个大的训练数据集，我们可以使用基于原则的统计方法，如得分匹配，来训练我们的得分模型，以估计潜在的得分函数。为了构建我们的生成模型，我们必须找到一种方法，通过得分函数的向量场来生成新的数据点。假设我们已经给定了得分函数。想象一下有许多随机点散布在楼梯上。我们能否通过移动这些随机点来从得分函数中形成样本呢？可以通过跟随得分函数预测的方向来移动这些点。然而，这并不能给我们有效的样本，因为这些点最终会聚集在一起。将高斯噪声注入到得分函数中，并跟随这些噪声扰动的得分函数。 得分匹配可以估计数据的得分函数，Langevin 动力学可以从得分函数中生成样本。所以，用我们的得分模型替换 Langevin 动力学中的得分函数就变得非常自然了，然后我们就可以生成数据样本——从而定义一个新的生成模型。
Langevin 动力学：
score matching can estimate the score function data. And Langevin dynamics can generate samples from the score function. So it becomes very, very natural to just replace the score function in Langevin dynamics with our score model, and then we can generate data samples–we define a new generative model."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-06T20:51:51+08:00"><meta property="article:modified_time" content="2025-11-06T20:51:51+08:00"><meta property="og:image" content="https://tyh382596868.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://tyh382596868.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Score Based Generative Models"><meta name=twitter:description content="https://www.youtube.com/watch?v=wMmqCMwuM2Q
https://www.youtube.com/watch?v=B4oHJpEJBAA
https://scorebasedgenerativemodeling.github.io/
https://yang-song.net/blog/2021/score/

使用“基于得分的生成模型”这个术语，是希望强调它们与得分函数之间的关系。
得分函数被定义为对数概率密度的梯度。
统计学和机器学习中的典型假设是，训练数据中的所有数据点都来自某种潜在的数据分布。换句话说，这些数据点基本上是从该数据分布中独立抽取的样本，但我们并没有数据分布的解析形式，我们需要对其进行估计。
为了估计这个数据分布，我们需要创建一个模型。这个模型表示了一个参数化的概率分布，我们称之为模型分布。我们希望调整这个模型的参数，以确保它在某种意义上接近数据分布。
如果模型分布非常接近数据分布，那么我们可以将这个模型用于许多重要的应用。例如，当然，我们可以通过从这个模型分布中采样来生成无限数量的新的数据点。
另一个应用是，我们可以使用这个模型分布来计算任何潜在数据点的概率值。举个例子，对于一个数据点，比如一张吉娃娃的照片，因为它是一张狗的照片，所以它实际上属于我们的数据分布。因此，这个模型分布通常会为这样的数据点分配较高的概率值。对于一些不相关的数据点，比如一张松饼的照片，因为它不是狗的照片，一个好的模型分布会为这种图片分配较低的概率值。因为这个模型分布提供了一种生成新的数据点的方式，所以我们也将其称为生成模型。
模型提供了一组概率分布族。我们希望通过最小化 $𝑃_𝜃$与 $𝑃_{data}$ 之间的距离，从这个庞大的分布族中找到一个特定的概率分布。之后，我们就可以从 $𝑃_{data}$ 中生成样本。
数据分布可能非常复杂，尤其是当数据维度很高时。因此，我们必须构建一个非常强大的模型分布，才能估计我们的数据分布。那么，如何构建一个强大的模型分布呢？
得分函数：得分函数用于表示我们的概率分布。那么，什么是得分函数呢？假设我们有一个连续的概率分布，使用 𝑝(𝑥) 来表示概率密度函数。我们将得分函数定义为对 log⁡𝑝(𝑥) 的梯度。注意这个梯度是相对于随机变量 𝑥 计算的——它不是相对于任何模型参数（如 𝜃）计算的。
score function:从直觉上来说，分数函数也非常有意义：它告诉你，为了让某个数据点的概率变高，你应该往哪个方向移动它。真实的分数函数可以告诉你，在数据空间中的任意点，你应当往哪个方向移动，才能更接近真实数据点。
得分函数长什么样子呢？我们来看一个简单的例子，即两个高斯分布的混合。这个图展示了这个高斯混合分布的密度函数和得分函数。密度函数用颜色编码，颜色越深表示密度越高。得分函数是一个向量场，给出了密度函数增长最快的方向。给定密度函数，我们可以非常容易地计算得分函数，因为我们只需要对其进行求导。相反，利用得分函数，我们也可以通过计算积分来恢复密度函数。因此它们在理论上是等价的。但是在计算上，与密度函数相比，得分函数要更容易处理。

得分模型：根据我们在统计学中的知识，我们知道可以训练一个适当归一化的统计模型，使用如最大似然估计等方法来估计潜在的数据密度。但因为我们想使用得分函数，我们希望开发一个类似的方法，使我们能够训练我们的得分模型来估计潜在的得分函数，且只需一个有限的训练数据集。
从数学角度来看，我们给定一组数据点，假设这些数据点是从数据分布 $P_{\text{data}}​$ 中独立采样的。我们的目标是估计数据密度的得分函数。我们给定一个得分模型，这个模型假设是一个深度神经网络模型，能够将高维输入映射到高维输出，并且我们希望训练这个得分模型，使其近似于数据分布的真实得分函数。那么，如何训练这个得分模型，使其接近我们的真实数据得分函数呢？我们需要最小化某个目标函数。这个目标函数必须比较得分函数的两个向量场。这里，一个向量场是数据得分函数的真实值，另一个向量场是由我们的得分模型预测的。我们如何比较它们之间的差异呢？我们回顾一下，这两个向量场实际上位于相同的空间中。因此，我们可能能够计算这些向量对之间的差异向量，然后我们可以对这些差异向量的密度进行积分，形成一个标量值的目标函数。

Fisher divergence:
score matching:指的是用我们预测的分数 $𝑠_𝜃(𝑥)$去逼近原始分布的真实分数。这基本上意味着我们试图最小化真实分数和预测分数之间的差异
雅可比矩阵:
切片得分匹配（sliced score matching）: 基本的直觉是，一维问题应该比高维问题更容易解决。那么，我们如何将高维问题转换为一维问题呢？我们可以利用随机投影。我们将高维向量场投影到随机方向上。然后我们得到一维标量场。假设这两个高维向量场相互接近。那么我们可以沿着随机的一维方向投影它们。这样，我们就得到了接近的标量场。
denoising score matching：模型在训练时需要从带噪输入中预测负噪声。而预测负噪声的方向恰好是 朝向真实数据分布的方向——这正是“分数”的含义：告诉你朝哪里走概率更大。

给定一个大的训练数据集，我们可以使用基于原则的统计方法，如得分匹配，来训练我们的得分模型，以估计潜在的得分函数。为了构建我们的生成模型，我们必须找到一种方法，通过得分函数的向量场来生成新的数据点。假设我们已经给定了得分函数。想象一下有许多随机点散布在楼梯上。我们能否通过移动这些随机点来从得分函数中形成样本呢？可以通过跟随得分函数预测的方向来移动这些点。然而，这并不能给我们有效的样本，因为这些点最终会聚集在一起。将高斯噪声注入到得分函数中，并跟随这些噪声扰动的得分函数。

得分匹配可以估计数据的得分函数，Langevin 动力学可以从得分函数中生成样本。所以，用我们的得分模型替换 Langevin 动力学中的得分函数就变得非常自然了，然后我们就可以生成数据样本——从而定义一个新的生成模型。
Langevin 动力学：
score matching can estimate the score function data. And Langevin dynamics can generate samples from the score function. So it becomes very, very natural to just replace the score
function in Langevin dynamics with our score model, and then we can generate data samples&ndash;we define a new generative model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tyh382596868.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Score Based Generative Models","item":"https://tyh382596868.github.io/posts/score-based_generative_models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Score Based Generative Models","name":"Score Based Generative Models","description":"https://www.youtube.com/watch?v=wMmqCMwuM2Q\nhttps://www.youtube.com/watch?v=B4oHJpEJBAA\nhttps://scorebasedgenerativemodeling.github.io/\nhttps://yang-song.net/blog/2021/score/\n使用“基于得分的生成模型”这个术语，是希望强调它们与得分函数之间的关系。\n得分函数被定义为对数概率密度的梯度。\n统计学和机器学习中的典型假设是，训练数据中的所有数据点都来自某种潜在的数据分布。换句话说，这些数据点基本上是从该数据分布中独立抽取的样本，但我们并没有数据分布的解析形式，我们需要对其进行估计。\n为了估计这个数据分布，我们需要创建一个模型。这个模型表示了一个参数化的概率分布，我们称之为模型分布。我们希望调整这个模型的参数，以确保它在某种意义上接近数据分布。\n如果模型分布非常接近数据分布，那么我们可以将这个模型用于许多重要的应用。例如，当然，我们可以通过从这个模型分布中采样来生成无限数量的新的数据点。\n另一个应用是，我们可以使用这个模型分布来计算任何潜在数据点的概率值。举个例子，对于一个数据点，比如一张吉娃娃的照片，因为它是一张狗的照片，所以它实际上属于我们的数据分布。因此，这个模型分布通常会为这样的数据点分配较高的概率值。对于一些不相关的数据点，比如一张松饼的照片，因为它不是狗的照片，一个好的模型分布会为这种图片分配较低的概率值。因为这个模型分布提供了一种生成新的数据点的方式，所以我们也将其称为生成模型。\n模型提供了一组概率分布族。我们希望通过最小化 $𝑃_𝜃$与 $𝑃_{data}$ 之间的距离，从这个庞大的分布族中找到一个特定的概率分布。之后，我们就可以从 $𝑃_{data}$ 中生成样本。\n数据分布可能非常复杂，尤其是当数据维度很高时。因此，我们必须构建一个非常强大的模型分布，才能估计我们的数据分布。那么，如何构建一个强大的模型分布呢？\n得分函数：得分函数用于表示我们的概率分布。那么，什么是得分函数呢？假设我们有一个连续的概率分布，使用 𝑝(𝑥) 来表示概率密度函数。我们将得分函数定义为对 log⁡𝑝(𝑥) 的梯度。注意这个梯度是相对于随机变量 𝑥 计算的——它不是相对于任何模型参数（如 𝜃）计算的。\nscore function:从直觉上来说，分数函数也非常有意义：它告诉你，为了让某个数据点的概率变高，你应该往哪个方向移动它。真实的分数函数可以告诉你，在数据空间中的任意点，你应当往哪个方向移动，才能更接近真实数据点。\n得分函数长什么样子呢？我们来看一个简单的例子，即两个高斯分布的混合。这个图展示了这个高斯混合分布的密度函数和得分函数。密度函数用颜色编码，颜色越深表示密度越高。得分函数是一个向量场，给出了密度函数增长最快的方向。给定密度函数，我们可以非常容易地计算得分函数，因为我们只需要对其进行求导。相反，利用得分函数，我们也可以通过计算积分来恢复密度函数。因此它们在理论上是等价的。但是在计算上，与密度函数相比，得分函数要更容易处理。 得分模型：根据我们在统计学中的知识，我们知道可以训练一个适当归一化的统计模型，使用如最大似然估计等方法来估计潜在的数据密度。但因为我们想使用得分函数，我们希望开发一个类似的方法，使我们能够训练我们的得分模型来估计潜在的得分函数，且只需一个有限的训练数据集。\n从数学角度来看，我们给定一组数据点，假设这些数据点是从数据分布 $P_{\\text{data}}​$ 中独立采样的。我们的目标是估计数据密度的得分函数。我们给定一个得分模型，这个模型假设是一个深度神经网络模型，能够将高维输入映射到高维输出，并且我们希望训练这个得分模型，使其近似于数据分布的真实得分函数。那么，如何训练这个得分模型，使其接近我们的真实数据得分函数呢？我们需要最小化某个目标函数。这个目标函数必须比较得分函数的两个向量场。这里，一个向量场是数据得分函数的真实值，另一个向量场是由我们的得分模型预测的。我们如何比较它们之间的差异呢？我们回顾一下，这两个向量场实际上位于相同的空间中。因此，我们可能能够计算这些向量对之间的差异向量，然后我们可以对这些差异向量的密度进行积分，形成一个标量值的目标函数。 Fisher divergence:\nscore matching:指的是用我们预测的分数 $𝑠_𝜃(𝑥)$去逼近原始分布的真实分数。这基本上意味着我们试图最小化真实分数和预测分数之间的差异\n雅可比矩阵:\n切片得分匹配（sliced score matching）: 基本的直觉是，一维问题应该比高维问题更容易解决。那么，我们如何将高维问题转换为一维问题呢？我们可以利用随机投影。我们将高维向量场投影到随机方向上。然后我们得到一维标量场。假设这两个高维向量场相互接近。那么我们可以沿着随机的一维方向投影它们。这样，我们就得到了接近的标量场。\ndenoising score matching：模型在训练时需要从带噪输入中预测负噪声。而预测负噪声的方向恰好是 朝向真实数据分布的方向——这正是“分数”的含义：告诉你朝哪里走概率更大。 给定一个大的训练数据集，我们可以使用基于原则的统计方法，如得分匹配，来训练我们的得分模型，以估计潜在的得分函数。为了构建我们的生成模型，我们必须找到一种方法，通过得分函数的向量场来生成新的数据点。假设我们已经给定了得分函数。想象一下有许多随机点散布在楼梯上。我们能否通过移动这些随机点来从得分函数中形成样本呢？可以通过跟随得分函数预测的方向来移动这些点。然而，这并不能给我们有效的样本，因为这些点最终会聚集在一起。将高斯噪声注入到得分函数中，并跟随这些噪声扰动的得分函数。 得分匹配可以估计数据的得分函数，Langevin 动力学可以从得分函数中生成样本。所以，用我们的得分模型替换 Langevin 动力学中的得分函数就变得非常自然了，然后我们就可以生成数据样本——从而定义一个新的生成模型。\nLangevin 动力学：\nscore matching can estimate the score function data. And Langevin dynamics can generate samples from the score function. So it becomes very, very natural to just replace the score function in Langevin dynamics with our score model, and then we can generate data samples\u0026ndash;we define a new generative model.\n","keywords":[],"articleBody":"https://www.youtube.com/watch?v=wMmqCMwuM2Q\nhttps://www.youtube.com/watch?v=B4oHJpEJBAA\nhttps://scorebasedgenerativemodeling.github.io/\nhttps://yang-song.net/blog/2021/score/\n使用“基于得分的生成模型”这个术语，是希望强调它们与得分函数之间的关系。\n得分函数被定义为对数概率密度的梯度。\n统计学和机器学习中的典型假设是，训练数据中的所有数据点都来自某种潜在的数据分布。换句话说，这些数据点基本上是从该数据分布中独立抽取的样本，但我们并没有数据分布的解析形式，我们需要对其进行估计。\n为了估计这个数据分布，我们需要创建一个模型。这个模型表示了一个参数化的概率分布，我们称之为模型分布。我们希望调整这个模型的参数，以确保它在某种意义上接近数据分布。\n如果模型分布非常接近数据分布，那么我们可以将这个模型用于许多重要的应用。例如，当然，我们可以通过从这个模型分布中采样来生成无限数量的新的数据点。\n另一个应用是，我们可以使用这个模型分布来计算任何潜在数据点的概率值。举个例子，对于一个数据点，比如一张吉娃娃的照片，因为它是一张狗的照片，所以它实际上属于我们的数据分布。因此，这个模型分布通常会为这样的数据点分配较高的概率值。对于一些不相关的数据点，比如一张松饼的照片，因为它不是狗的照片，一个好的模型分布会为这种图片分配较低的概率值。因为这个模型分布提供了一种生成新的数据点的方式，所以我们也将其称为生成模型。\n模型提供了一组概率分布族。我们希望通过最小化 $𝑃_𝜃$与 $𝑃_{data}$ 之间的距离，从这个庞大的分布族中找到一个特定的概率分布。之后，我们就可以从 $𝑃_{data}$ 中生成样本。\n数据分布可能非常复杂，尤其是当数据维度很高时。因此，我们必须构建一个非常强大的模型分布，才能估计我们的数据分布。那么，如何构建一个强大的模型分布呢？\n得分函数：得分函数用于表示我们的概率分布。那么，什么是得分函数呢？假设我们有一个连续的概率分布，使用 𝑝(𝑥) 来表示概率密度函数。我们将得分函数定义为对 log⁡𝑝(𝑥) 的梯度。注意这个梯度是相对于随机变量 𝑥 计算的——它不是相对于任何模型参数（如 𝜃）计算的。\nscore function:从直觉上来说，分数函数也非常有意义：它告诉你，为了让某个数据点的概率变高，你应该往哪个方向移动它。真实的分数函数可以告诉你，在数据空间中的任意点，你应当往哪个方向移动，才能更接近真实数据点。\n得分函数长什么样子呢？我们来看一个简单的例子，即两个高斯分布的混合。这个图展示了这个高斯混合分布的密度函数和得分函数。密度函数用颜色编码，颜色越深表示密度越高。得分函数是一个向量场，给出了密度函数增长最快的方向。给定密度函数，我们可以非常容易地计算得分函数，因为我们只需要对其进行求导。相反，利用得分函数，我们也可以通过计算积分来恢复密度函数。因此它们在理论上是等价的。但是在计算上，与密度函数相比，得分函数要更容易处理。 得分模型：根据我们在统计学中的知识，我们知道可以训练一个适当归一化的统计模型，使用如最大似然估计等方法来估计潜在的数据密度。但因为我们想使用得分函数，我们希望开发一个类似的方法，使我们能够训练我们的得分模型来估计潜在的得分函数，且只需一个有限的训练数据集。\n从数学角度来看，我们给定一组数据点，假设这些数据点是从数据分布 $P_{\\text{data}}​$ 中独立采样的。我们的目标是估计数据密度的得分函数。我们给定一个得分模型，这个模型假设是一个深度神经网络模型，能够将高维输入映射到高维输出，并且我们希望训练这个得分模型，使其近似于数据分布的真实得分函数。那么，如何训练这个得分模型，使其接近我们的真实数据得分函数呢？我们需要最小化某个目标函数。这个目标函数必须比较得分函数的两个向量场。这里，一个向量场是数据得分函数的真实值，另一个向量场是由我们的得分模型预测的。我们如何比较它们之间的差异呢？我们回顾一下，这两个向量场实际上位于相同的空间中。因此，我们可能能够计算这些向量对之间的差异向量，然后我们可以对这些差异向量的密度进行积分，形成一个标量值的目标函数。 Fisher divergence:\nscore matching:指的是用我们预测的分数 $𝑠_𝜃(𝑥)$去逼近原始分布的真实分数。这基本上意味着我们试图最小化真实分数和预测分数之间的差异\n雅可比矩阵:\n切片得分匹配（sliced score matching）: 基本的直觉是，一维问题应该比高维问题更容易解决。那么，我们如何将高维问题转换为一维问题呢？我们可以利用随机投影。我们将高维向量场投影到随机方向上。然后我们得到一维标量场。假设这两个高维向量场相互接近。那么我们可以沿着随机的一维方向投影它们。这样，我们就得到了接近的标量场。\ndenoising score matching：模型在训练时需要从带噪输入中预测负噪声。而预测负噪声的方向恰好是 朝向真实数据分布的方向——这正是“分数”的含义：告诉你朝哪里走概率更大。 给定一个大的训练数据集，我们可以使用基于原则的统计方法，如得分匹配，来训练我们的得分模型，以估计潜在的得分函数。为了构建我们的生成模型，我们必须找到一种方法，通过得分函数的向量场来生成新的数据点。假设我们已经给定了得分函数。想象一下有许多随机点散布在楼梯上。我们能否通过移动这些随机点来从得分函数中形成样本呢？可以通过跟随得分函数预测的方向来移动这些点。然而，这并不能给我们有效的样本，因为这些点最终会聚集在一起。将高斯噪声注入到得分函数中，并跟随这些噪声扰动的得分函数。 得分匹配可以估计数据的得分函数，Langevin 动力学可以从得分函数中生成样本。所以，用我们的得分模型替换 Langevin 动力学中的得分函数就变得非常自然了，然后我们就可以生成数据样本——从而定义一个新的生成模型。\nLangevin 动力学：\nscore matching can estimate the score function data. And Langevin dynamics can generate samples from the score function. So it becomes very, very natural to just replace the score function in Langevin dynamics with our score model, and then we can generate data samples–we define a new generative model.\nNoise Perturbation：为数据加上不同程度的噪声，让数据分布更多的空间。 Multiple Noise Perturbations：面临一个权衡问题：添加大噪声虽然能覆盖更多的数据空间，但也会改变数据的分布。如果我们选择一个非常低的 𝜎，那么当我们计算 $\\tilde{x}$时，噪声 𝜖 的标准差会非常小，显然它只包含非常小的值。可以做两者，或者说介于两者之间。我们可以在高噪声、中等噪声和低噪声水平下训练模型。简单地在一个预定义的范围内变化 𝜎，比如从 0.01 到 25 之间。这样，模型就能看到数据空间中的各个区域。根据噪声水平来调节模型，这是额外的自由信息，可能有助于模型在预测分数时做得更好。唯一的变化是 𝜎 的索引，这意味着它不再是一个固定的单一值，而是不同噪声值的序列，包括低噪声和高噪声。模型会在高 𝜎 时看到数据空间的大部分区域，但在 𝜎 小时也能看到清晰的数据分布。这样，我们就能同时获得两者的优点。\n当你将噪声水平的数量增加到无限大时，噪声扰动变成了一个随机过程。使用一个高级的术语——随机微分方程（SDE） 来建模这些随机过程。 噪声数据密度的得分函数更容易准确估计，而且这些噪声数据密度的得分函数可能为指导 Langevin 动力学从低数据密度区域移动到高数据密度区域提供有价值的方向信息。但单纯地注入高斯噪声并不能解决所有问题。由于数据点的扰动，这些噪声数据密度不再是对原始真实数据密度的良好近似。\n使用不同噪声水平的多重序列。使用均值为 0，标准差从 $𝜎_1$ 到 $𝜎_3$ 的高斯噪声来扰动我们的训练数据集。这将给我们三个带噪声的训练数据集。对于每个带噪声的数据集，将有一个相应的噪声数据密度，我们用 $𝑝_{𝜎_1}$ 到 $𝑝_{𝜎3}$ 来表示。我们希望估计对应噪声密度的潜在函数。如何估计三个带噪声的得分函数呢？最直观的方法是我们训练三个网络，每个网络负责估计单一噪声水平的得分函数。一个更具可扩展性的解决方案是考虑一个条件得分模型，我们称之为噪声条件得分模型。将噪声水平 𝜎 作为一个额外的输入维度加入到模型中。输出对应于扰动了噪声水平 𝜎 的数据密度的得分函数。我们有一个得分匹配损失的求和项。对于每个噪声水平 $𝜎_𝑖$，我们有一个得分匹配损失，并且我们有一个正的加权函数 $𝜆𝜎_𝑖$。得分模型与扩散模型之间的联系首次在 2020 年的 DDPM 论文中揭示。回到如何在训练了得分匹配损失后从噪声条件得分模型中采样的问题。仍然可以使用 Langevin 动力学。首先应用 Langevin 动力学从最大的扰动噪声的得分模型中进行采样。这些样本将作为初始化，接着从下一个噪声水平的得分模型中进行采样。以这种方式继续，直到最终从具有最小噪声水平的得分函数中生成样本。\n假设我们有一个无条件（unconditional）的 score-based 生成模型，它能够同时生成狗和猫的图像。但现在我们只想生成狗的图像。那么该怎么做呢？我们假设有一个前向模型（forward model），它是一个图像分类器：给定图片 x 输出其标签 y。 我们希望使用一个控制信号，即目标标签 y，并将其指定为“狗”。目标是从条件分布 p(x∣y)p(x|y)p(x∣y) 中采样。 这个条件分布就只会产生狗的图像。我们将其称为逆分布（inverse distribution），因为它可以被视为前向模型的概率反演。那么我们如何获得这个逆分布呢？ 常见方法是使用 Bayes 定理。其中我们已知无条件分布 𝑝(𝑥)，前向模型提供 𝑝(𝑦∣𝑥)，但分母 𝑝(𝑦) 是一个归一化常数，难以直接求出。这个问题可以通过 score 函数 来规避。我们只需要对 Bayes 定理两边取对数，然后再对 𝑥 求梯度。由于归一化常数与 𝑥 无关，其梯度为零，从而消失。 也就是说，条件分布的 score 函数是两个部分的和：\n无条件 score 函数（可以通过训练好的无条件 score 模型获得） 前向模型对数概率的梯度（例如分类器梯度，可直接反向传播求得） 其好处是：\n我们 只需要训练一次无条件模型，然后通过改变前向模型，就能实现多种条件生成应用。 stochastic process：从原始的干净数据点开始，逐渐注入高斯噪声，使数据不断被扰动。随着噪声注入的不断累积，最终我们会得到非常嘈杂的图像，它们接近于来自某个简单高斯分布的样本。这些带噪声的数据在噪声注入过程中所形成的轨迹，就是一个随机过程的轨迹。 一个随机过程可以理解为：由无限多个随机变量组成的集合，而这些随机变量由一个连续参数 ttt 进行索引。对于每一个随机变量，都会对应一个概率密度分布。因此，一个随机过程就对应了无限多个概率密度函数。\n那么，我们如何选择一个合适的随机过程，使它能够表示这无限多个带噪声的数据分布呢？ 答案是引入随机微分方程（Stochastic Differential Equation, SDE）。\n随机微分方程与普通微分方程非常相似，但它多了一个用来引入随机性（噪声）的项。 reverse stochastic process： Euler-Maruyama approach:\nKL divergence:\n前向模型（forward model）与score 函数之间的联系。比如你说前向模型可以是物理模拟、也可以是图像转文本网络。那么前向模型有什么要求？它的限制是什么？ 理想情况下，前向模型和 score 函数一样，也需要依赖噪声水平 𝑡。 如果前向模型能够根据噪声水平进行条件化，那么我们就能直接、严格地从条件分布中采样，例如文本到图像生成就是这样实现的——这些模型中的前向模型都是随时间（噪声水平）变化的。如果前向模型不依赖噪声水平，就像医疗图像重建中的情况，我们就需要做近似。 如果前向模型是线性的，近似通常比较容易，并且现在已经有一系列工作提供了有效的近似方法。 如果前向模型是非线性的，问题会更复杂一些，但仍然是可行的，也有相关研究。目前仍然有一个研究方向在探索： 是否存在一种有原则的方法，可以将一个单一的前向模型转换为一系列依赖噪声水平 𝑡 的前向模型。\nprobability density function：PDF is all you need to sample from a distribution。概率密度函数。告诉我们。我们需要的数据（如猫的图片，狗的图片）在空间中的位置。 ","wordCount":"235","inLanguage":"en","image":"https://tyh382596868.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-11-06T20:51:51+08:00","dateModified":"2025-11-06T20:51:51+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tyh382596868.github.io/posts/score-based_generative_models/"},"publisher":{"@type":"Organization","name":"tyh's blog","logo":{"@type":"ImageObject","url":"https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://tyh382596868.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://tyh382596868.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tyh382596868.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://tyh382596868.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://tyh382596868.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://tyh382596868.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Score Based Generative Models</h1><div class=post-meta><span title='2025-11-06 20:51:51 +0800 CST'>November 6, 2025</span>&nbsp;·&nbsp;<span>2 min</span>&nbsp;·&nbsp;<span>235 words</span>&nbsp;·&nbsp;<span>Me</span>&nbsp;|&nbsp;<span>
<a href=https://github.com/%3cpath_to_repo%3e/content/posts/Score-Based_Generative_Models.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></span></div></header><div class=post-content><p><a href="https://www.youtube.com/watch?v=wMmqCMwuM2Q">https://www.youtube.com/watch?v=wMmqCMwuM2Q</a></p><p><a href="https://www.youtube.com/watch?v=B4oHJpEJBAA">https://www.youtube.com/watch?v=B4oHJpEJBAA</a></p><p><a href=https://scorebasedgenerativemodeling.github.io/>https://scorebasedgenerativemodeling.github.io/</a></p><p><a href=https://yang-song.net/blog/2021/score/>https://yang-song.net/blog/2021/score/</a></p><hr><p>使用“基于得分的生成模型”这个术语，是希望强调它们与得分函数之间的关系。</p><p>得分函数被定义为对数概率密度的梯度。</p><p>统计学和机器学习中的典型假设是，训练数据中的所有数据点都来自某种潜在的数据分布。换句话说，这些数据点基本上是从该数据分布中独立抽取的样本，但我们并没有数据分布的解析形式，我们需要对其进行估计。</p><p>为了估计这个数据分布，我们需要创建一个模型。这个模型表示了一个参数化的概率分布，我们称之为模型分布。我们希望调整这个模型的参数，以确保它在某种意义上接近数据分布。</p><p>如果模型分布非常接近数据分布，那么我们可以将这个模型用于许多重要的应用。例如，当然，我们可以通过从这个模型分布中采样来生成无限数量的新的数据点。</p><p>另一个应用是，我们可以使用这个模型分布来计算任何潜在数据点的概率值。举个例子，对于一个数据点，比如一张吉娃娃的照片，因为它是一张狗的照片，所以它实际上属于我们的数据分布。因此，这个模型分布通常会为这样的数据点分配较高的概率值。对于一些不相关的数据点，比如一张松饼的照片，因为它不是狗的照片，一个好的模型分布会为这种图片分配较低的概率值。因为这个模型分布提供了一种生成新的数据点的方式，所以我们也将其称为生成模型。</p><p>模型提供了一组概率分布族。我们希望通过最小化 $𝑃_𝜃$与 $𝑃_{data}$ 之间的距离，从这个庞大的分布族中找到一个特定的概率分布。之后，我们就可以从 $𝑃_{data}$ 中生成样本。</p><p>数据分布可能非常复杂，尤其是当数据维度很高时。因此，我们必须构建一个非常强大的模型分布，才能估计我们的数据分布。那么，如何构建一个强大的模型分布呢？</p><p>得分函数：得分函数用于表示我们的概率分布。那么，什么是得分函数呢？假设我们有一个连续的概率分布，使用 𝑝(𝑥) 来表示概率密度函数。我们将得分函数定义为对 log⁡𝑝(𝑥) 的梯度。注意这个梯度是相对于随机变量 𝑥 计算的——它不是相对于任何模型参数（如 𝜃）计算的。</p><p>score function:从直觉上来说，分数函数也非常有意义：它告诉你，为了让某个数据点的概率变高，你应该往哪个方向移动它。真实的分数函数可以告诉你，在数据空间中的任意点，你应当往哪个方向移动，才能更接近真实数据点。</p><p>得分函数长什么样子呢？我们来看一个简单的例子，即两个高斯分布的混合。这个图展示了这个高斯混合分布的密度函数和得分函数。密度函数用颜色编码，颜色越深表示密度越高。得分函数是一个向量场，给出了密度函数增长最快的方向。给定密度函数，我们可以非常容易地计算得分函数，因为我们只需要对其进行求导。相反，利用得分函数，我们也可以通过计算积分来恢复密度函数。因此它们在理论上是等价的。但是在计算上，与密度函数相比，得分函数要更容易处理。
<img loading=lazy src=/attachment/eba18f8093138db297ea4d6b6a78350e.png></p><p>得分模型：根据我们在统计学中的知识，我们知道可以训练一个适当归一化的统计模型，使用如最大似然估计等方法来估计潜在的数据密度。但因为我们想使用得分函数，我们希望开发一个类似的方法，使我们能够训练我们的得分模型来估计潜在的得分函数，且只需一个有限的训练数据集。</p><p>从数学角度来看，我们给定一组数据点，假设这些数据点是从数据分布 $P_{\text{data}}​$ 中独立采样的。我们的目标是估计数据密度的得分函数。我们给定一个得分模型，这个模型假设是一个深度神经网络模型，能够将高维输入映射到高维输出，并且我们希望训练这个得分模型，使其近似于数据分布的真实得分函数。那么，如何训练这个得分模型，使其接近我们的真实数据得分函数呢？我们需要最小化某个目标函数。这个目标函数必须比较得分函数的两个向量场。这里，一个向量场是数据得分函数的真实值，另一个向量场是由我们的得分模型预测的。我们如何比较它们之间的差异呢？我们回顾一下，这两个向量场实际上位于相同的空间中。因此，我们可能能够计算这些向量对之间的差异向量，然后我们可以对这些差异向量的密度进行积分，形成一个标量值的目标函数。
<img loading=lazy src=/attachment/fb33945828784ed2ac22b9428dd85181.png>
Fisher divergence:</p><p>score matching:指的是用我们预测的分数 $𝑠_𝜃(𝑥)$去逼近原始分布的真实分数。这基本上意味着我们试图最小化真实分数和预测分数之间的差异</p><p>雅可比矩阵:</p><p>切片得分匹配（sliced score matching）: 基本的直觉是，一维问题应该比高维问题更容易解决。那么，我们如何将高维问题转换为一维问题呢？我们可以利用随机投影。我们将高维向量场投影到随机方向上。然后我们得到一维标量场。假设这两个高维向量场相互接近。那么我们可以沿着随机的一维方向投影它们。这样，我们就得到了接近的标量场。</p><p>denoising score matching：模型在训练时需要从带噪输入中预测负噪声。而预测负噪声的方向恰好是 朝向真实数据分布的方向——这正是“分数”的含义：告诉你朝哪里走概率更大。
<img loading=lazy src=/attachment/74ce29358bb6edc7fcd8bebe66960248.png>
给定一个大的训练数据集，我们可以使用基于原则的统计方法，如得分匹配，来训练我们的得分模型，以估计潜在的得分函数。为了构建我们的生成模型，我们必须找到一种方法，通过得分函数的向量场来生成新的数据点。假设我们已经给定了得分函数。想象一下有许多随机点散布在楼梯上。我们能否通过移动这些随机点来从得分函数中形成样本呢？可以通过跟随得分函数预测的方向来移动这些点。然而，这并不能给我们有效的样本，因为这些点最终会聚集在一起。将高斯噪声注入到得分函数中，并跟随这些噪声扰动的得分函数。
<img loading=lazy src=/attachment/4357572873230a1e389e819d78b8d96f.png></p><p>得分匹配可以估计数据的得分函数，Langevin 动力学可以从得分函数中生成样本。所以，用我们的得分模型替换 Langevin 动力学中的得分函数就变得非常自然了，然后我们就可以生成数据样本——从而定义一个新的生成模型。</p><p>Langevin 动力学：</p><p>score matching can estimate the score function data. And Langevin dynamics can generate samples from the score function. So it becomes very, very natural to just replace the score
function in Langevin dynamics with our score model, and then we can generate data samples&ndash;we define a new generative model.</p><p>Noise Perturbation：为数据加上不同程度的噪声，让数据分布更多的空间。
<img loading=lazy src=/attachment/731a89c865e1de2c8147be869fa7c525.png></p><p>Multiple Noise Perturbations：面临一个权衡问题：添加大噪声虽然能覆盖更多的数据空间，但也会改变数据的分布。如果我们选择一个非常低的 𝜎，那么当我们计算 $\tilde{x}$时，噪声 𝜖 的标准差会非常小，显然它只包含非常小的值。可以做两者，或者说介于两者之间。我们可以在高噪声、中等噪声和低噪声水平下训练模型。简单地在一个预定义的范围内变化 𝜎，比如从 0.01 到 25 之间。这样，模型就能看到数据空间中的各个区域。根据噪声水平来调节模型，这是额外的自由信息，可能有助于模型在预测分数时做得更好。唯一的变化是 𝜎 的索引，这意味着它不再是一个固定的单一值，而是不同噪声值的序列，包括低噪声和高噪声。模型会在高 𝜎 时看到数据空间的大部分区域，但在 𝜎 小时也能看到清晰的数据分布。这样，我们就能同时获得两者的优点。</p><p>当你将噪声水平的数量增加到无限大时，噪声扰动变成了一个随机过程。使用一个高级的术语——随机微分方程（SDE） 来建模这些随机过程。
<img loading=lazy src=/attachment/309510c137366122343a91c3e570e174.png></p><p>噪声数据密度的得分函数更容易准确估计，而且这些噪声数据密度的得分函数可能为指导 Langevin 动力学从低数据密度区域移动到高数据密度区域提供有价值的方向信息。但单纯地注入高斯噪声并不能解决所有问题。由于数据点的扰动，这些噪声数据密度不再是对原始真实数据密度的良好近似。</p><p>使用不同噪声水平的多重序列。使用均值为 0，标准差从 $𝜎_1$ 到 $𝜎_3$ 的高斯噪声来扰动我们的训练数据集。这将给我们三个带噪声的训练数据集。对于每个带噪声的数据集，将有一个相应的噪声数据密度，我们用 $𝑝_{𝜎_1}$ 到 $𝑝_{𝜎3}$ 来表示。我们希望估计对应噪声密度的潜在函数。如何估计三个带噪声的得分函数呢？最直观的方法是我们训练三个网络，每个网络负责估计单一噪声水平的得分函数。一个更具可扩展性的解决方案是考虑一个条件得分模型，我们称之为噪声条件得分模型。将噪声水平 𝜎 作为一个额外的输入维度加入到模型中。输出对应于扰动了噪声水平 𝜎 的数据密度的得分函数。我们有一个得分匹配损失的求和项。对于每个噪声水平 $𝜎_𝑖$，我们有一个得分匹配损失，并且我们有一个正的加权函数 $𝜆𝜎_𝑖$。得分模型与扩散模型之间的联系首次在 2020 年的 DDPM 论文中揭示。回到如何在训练了得分匹配损失后从噪声条件得分模型中采样的问题。仍然可以使用 Langevin 动力学。首先应用 Langevin 动力学从最大的扰动噪声的得分模型中进行采样。这些样本将作为初始化，接着从下一个噪声水平的得分模型中进行采样。以这种方式继续，直到最终从具有最小噪声水平的得分函数中生成样本。<br><img loading=lazy src=/attachment/2c60446ec4e738a0e2ef446ac835ca77.png></p><p>假设我们有一个无条件（unconditional）的 score-based 生成模型，它能够同时生成狗和猫的图像。但现在我们只想生成狗的图像。那么该怎么做呢？我们假设有一个前向模型（forward model），它是一个图像分类器：给定图片 x 输出其标签 y。 我们希望使用一个控制信号，即目标标签 y，并将其指定为“狗”。目标是从条件分布 p(x∣y)p(x|y)p(x∣y) 中采样。 这个条件分布就只会产生狗的图像。我们将其称为<strong>逆分布（inverse distribution）</strong>，因为它可以被视为前向模型的概率反演。那么我们如何获得这个逆分布呢？ 常见方法是使用 Bayes 定理。其中我们已知无条件分布 𝑝(𝑥)，前向模型提供 𝑝(𝑦∣𝑥)，但分母 𝑝(𝑦) 是一个归一化常数，难以直接求出。这个问题可以通过 score 函数 来规避。我们只需要对 Bayes 定理两边取对数，然后再对 𝑥 求梯度。由于归一化常数与 𝑥 无关，其梯度为零，从而消失。
也就是说，<strong>条件分布的 score 函数</strong>是两个部分的和：</p><ol><li><strong>无条件 score 函数</strong>（可以通过训练好的无条件 score 模型获得）</li><li><strong>前向模型对数概率的梯度</strong>（例如分类器梯度，可直接反向传播求得）
其好处是：<br>我们 <strong>只需要训练一次无条件模型</strong>，然后通过改变前向模型，就能实现<strong>多种条件生成应用</strong>。
<img loading=lazy src=/attachment/dfee421d2bc3a1b0d7ba54162cd1af5f.png>
stochastic process：从原始的干净数据点开始，逐渐注入高斯噪声，使数据不断被扰动。随着噪声注入的不断累积，最终我们会得到非常嘈杂的图像，它们接近于来自某个简单高斯分布的样本。这些带噪声的数据在噪声注入过程中所形成的轨迹，就是一个<strong>随机过程的轨迹</strong>。</li></ol><p>一个随机过程可以理解为：由<strong>无限多个随机变量</strong>组成的集合，而这些随机变量由一个连续参数 ttt 进行索引。对于每一个随机变量，都会对应一个概率密度分布。因此，一个随机过程就对应了<strong>无限多个概率密度函数</strong>。</p><p>那么，我们如何选择一个合适的随机过程，使它能够表示这无限多个带噪声的数据分布呢？
答案是引入<strong>随机微分方程（Stochastic Differential Equation, SDE）</strong>。<br>随机微分方程与普通微分方程非常相似，但它多了一个用来引入随机性（噪声）的项。
<img loading=lazy src=/attachment/964372dc9f7c1b4deb02edfc6ffd6bbe.png>
reverse stochastic process：
<img loading=lazy src=/attachment/40bda0a4e2d4b98cadae89149b52c43c.png></p><p>Euler-Maruyama approach:</p><p>KL divergence:</p><p>前向模型（forward model）与score 函数之间的联系。比如你说前向模型可以是物理模拟、也可以是图像转文本网络。那么前向模型有什么要求？它的限制是什么？
理想情况下，前向模型和 score 函数一样，也需要依赖噪声水平 𝑡。
如果前向模型能够根据噪声水平进行条件化，那么我们就能直接、严格地从条件分布中采样，例如文本到图像生成就是这样实现的——这些模型中的前向模型都是随时间（噪声水平）变化的。如果前向模型不依赖噪声水平，就像医疗图像重建中的情况，我们就需要做近似。
如果前向模型是线性的，近似通常比较容易，并且现在已经有一系列工作提供了有效的近似方法。
如果前向模型是非线性的，问题会更复杂一些，但仍然是可行的，也有相关研究。目前仍然有一个研究方向在探索：
是否存在一种有原则的方法，可以将一个单一的前向模型转换为一系列依赖噪声水平 𝑡 的前向模型。</p><p>probability density function：PDF is all you need to sample from a distribution。概率密度函数。告诉我们。我们需要的数据（如猫的图片，狗的图片）在空间中的位置。
<img loading=lazy src=/attachment/dc9d10b5364949ce4cf156b957ec8eec.png></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://tyh382596868.github.io/posts/transformer/><span class=title>« Prev</span><br><span>Transformer</span>
</a><a class=next href=https://tyh382596868.github.io/posts/my-first-posts/><span class=title>Next »</span><br><span>Math Code Block</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Score Based Generative Models on x" href="https://x.com/intent/tweet/?text=Score%20Based%20Generative%20Models&amp;url=https%3a%2f%2ftyh382596868.github.io%2fposts%2fscore-based_generative_models%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Score Based Generative Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftyh382596868.github.io%2fposts%2fscore-based_generative_models%2f&amp;title=Score%20Based%20Generative%20Models&amp;summary=Score%20Based%20Generative%20Models&amp;source=https%3a%2f%2ftyh382596868.github.io%2fposts%2fscore-based_generative_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Score Based Generative Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftyh382596868.github.io%2fposts%2fscore-based_generative_models%2f&title=Score%20Based%20Generative%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Score Based Generative Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftyh382596868.github.io%2fposts%2fscore-based_generative_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Score Based Generative Models on whatsapp" href="https://api.whatsapp.com/send?text=Score%20Based%20Generative%20Models%20-%20https%3a%2f%2ftyh382596868.github.io%2fposts%2fscore-based_generative_models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Score Based Generative Models on telegram" href="https://telegram.me/share/url?text=Score%20Based%20Generative%20Models&amp;url=https%3a%2f%2ftyh382596868.github.io%2fposts%2fscore-based_generative_models%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Score Based Generative Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=Score%20Based%20Generative%20Models&u=https%3a%2f%2ftyh382596868.github.io%2fposts%2fscore-based_generative_models%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://tyh382596868.github.io/>tyh's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>