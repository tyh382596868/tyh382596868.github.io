<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformer | tyh's blog</title><meta name=keywords content><meta name=description content="Transformer
å¤§è¯­è¨€æ¨¡å‹è¦å…·æœ‰å¤æ‚ç†è§£ä»¥åŠç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸æ˜¯ä¸ºç‰¹å®šçš„è¯­è¨€ä»»åŠ¡æ‰€è®¾è®¡è€Œæ˜¯å…·æœ‰æ›´å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹çš„æˆåŠŸå½’å› äºTransformeræ¶æ„ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„æµ·é‡æ•°æ®ã€‚
é€šè¿‡ç¼–å†™ä»£ç åŸºäºTransformeræ¶æ„å®ç°ç±»ä¼¼ChatGPTçš„å¤§è¯­è¨€æ¨¡å‹ã€‚
å¤§è¯­è¨€æ¨¡å‹çš„å¤§æ—¢æŒ‡å‚æ•°é‡è§„æ¨¡ï¼ŒåˆæŒ‡æµ·é‡æ•°æ®ã€‚
Transformerå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯å®ƒèƒ½å¤Ÿé€‰æ‹©æ€§çš„å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚å…³é”®ç»„ä»¶æ˜¯self-attentionï¼Œèƒ½å¤Ÿè¡¡é‡è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªtokenç›¸å¯¹äºå…¶ä»–tokençš„ç›¸å¯¹é‡è¦æ€§ã€‚
å¤§è¯­è¨€æ¨¡å‹çš„æ„å»ºåŒ…å«pretraining and fine-tuningä¸¤ä¸ªé˜¶æ®µã€‚pretrainingé˜¶æ®µæ˜¯åœ¨æµ·é‡unlabeled text dataä¸Šè¿›è¡Œself-supervised learningï¼Œå»å­¦åˆ°ä¸€äº›general representionã€‚fine-tuningæ˜¯train on labeled dataã€‚
Transformeråˆ†ä¸ºencoderå’Œdecoderéƒ¨åˆ†ã€‚encoderè´Ÿè´£å°†è¾“å…¥åºåˆ—ç¼–ç æˆa series of numerical representations or vectors that capture the contextual information of the inputï¼Œè§£ç å™¨æ ¹æ®å½“å‰çš„these encoded vectorsä»¥åŠå½“å‰è¾“å…¥å®Œæˆä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ã€‚
å®ç°é¢„è®­ç»ƒçš„ä»£ç ã€å¤ç”¨å…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹
The next-word prediction taskæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦ä¸ºè®­ç»ƒæ•°æ®æä¾›æ ‡ç­¾ï¼Œä»–åˆ©ç”¨æ•°æ®è‡ªèº«çš„ç»“æ„ã€‚ç”¨æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªè¯ä½œä¸ºè¦è®­ç»ƒçš„æ ‡ç­¾ã€‚
Autoregressive modelsæ•´åˆæ—©å…ˆçš„è¾“å‡ºä½œä¸ºæœªæ¥é¢„æµ‹çš„è¾“å…¥ã€‚
original transformer çš„encoderå’Œdecoderé‡å¤6æ¬¡ã€‚GPT-3æœ‰96å±‚transformer layers ä»¥åŠ175bçš„parametersã€‚
emergent behaviorï¼šæ˜¯æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæœªè¢«æ˜¾ç¤ºè®­ç»ƒçš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚

Working with text data
ğŸthe required steps for preparing the embeddings used by an LLM
splitting text into individual word and subword tokens"><meta name=author content="Me"><link rel=canonical href=https://tyh382596868.github.io/posts/transformer/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.a5781257de4197b8e3d54346acdf1dd55a9ba4cb91dcace192fc1baf228c08b5.css integrity="sha256-pXgSV95Bl7jj1UNGrN8d1VqbpMuR3KzhkvwbryKMCLU=" rel="preload stylesheet" as=style><link rel=icon href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tyh382596868.github.io/posts/transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){if(typeof renderMathInElement!="function")return;renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredTags:["script","noscript","style","textarea","pre","code"],throwOnError:!1})})</script><meta property="og:url" content="https://tyh382596868.github.io/posts/transformer/"><meta property="og:site_name" content="tyh's blog"><meta property="og:title" content="Transformer"><meta property="og:description" content="Transformer å¤§è¯­è¨€æ¨¡å‹è¦å…·æœ‰å¤æ‚ç†è§£ä»¥åŠç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸æ˜¯ä¸ºç‰¹å®šçš„è¯­è¨€ä»»åŠ¡æ‰€è®¾è®¡è€Œæ˜¯å…·æœ‰æ›´å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹çš„æˆåŠŸå½’å› äºTransformeræ¶æ„ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„æµ·é‡æ•°æ®ã€‚
é€šè¿‡ç¼–å†™ä»£ç åŸºäºTransformeræ¶æ„å®ç°ç±»ä¼¼ChatGPTçš„å¤§è¯­è¨€æ¨¡å‹ã€‚
å¤§è¯­è¨€æ¨¡å‹çš„å¤§æ—¢æŒ‡å‚æ•°é‡è§„æ¨¡ï¼ŒåˆæŒ‡æµ·é‡æ•°æ®ã€‚
Transformerå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯å®ƒèƒ½å¤Ÿé€‰æ‹©æ€§çš„å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚å…³é”®ç»„ä»¶æ˜¯self-attentionï¼Œèƒ½å¤Ÿè¡¡é‡è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªtokenç›¸å¯¹äºå…¶ä»–tokençš„ç›¸å¯¹é‡è¦æ€§ã€‚
å¤§è¯­è¨€æ¨¡å‹çš„æ„å»ºåŒ…å«pretraining and fine-tuningä¸¤ä¸ªé˜¶æ®µã€‚pretrainingé˜¶æ®µæ˜¯åœ¨æµ·é‡unlabeled text dataä¸Šè¿›è¡Œself-supervised learningï¼Œå»å­¦åˆ°ä¸€äº›general representionã€‚fine-tuningæ˜¯train on labeled dataã€‚
Transformeråˆ†ä¸ºencoderå’Œdecoderéƒ¨åˆ†ã€‚encoderè´Ÿè´£å°†è¾“å…¥åºåˆ—ç¼–ç æˆa series of numerical representations or vectors that capture the contextual information of the inputï¼Œè§£ç å™¨æ ¹æ®å½“å‰çš„these encoded vectorsä»¥åŠå½“å‰è¾“å…¥å®Œæˆä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ã€‚
å®ç°é¢„è®­ç»ƒçš„ä»£ç ã€å¤ç”¨å…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹
The next-word prediction taskæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦ä¸ºè®­ç»ƒæ•°æ®æä¾›æ ‡ç­¾ï¼Œä»–åˆ©ç”¨æ•°æ®è‡ªèº«çš„ç»“æ„ã€‚ç”¨æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªè¯ä½œä¸ºè¦è®­ç»ƒçš„æ ‡ç­¾ã€‚
Autoregressive modelsæ•´åˆæ—©å…ˆçš„è¾“å‡ºä½œä¸ºæœªæ¥é¢„æµ‹çš„è¾“å…¥ã€‚
original transformer çš„encoderå’Œdecoderé‡å¤6æ¬¡ã€‚GPT-3æœ‰96å±‚transformer layers ä»¥åŠ175bçš„parametersã€‚
emergent behaviorï¼šæ˜¯æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæœªè¢«æ˜¾ç¤ºè®­ç»ƒçš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚
Working with text data ğŸthe required steps for preparing the embeddings used by an LLM
splitting text into individual word and subword tokens"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-17T12:42:22+08:00"><meta property="article:modified_time" content="2025-11-17T12:42:22+08:00"><meta property="og:image" content="https://tyh382596868.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://tyh382596868.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Transformer"><meta name=twitter:description content="Transformer
å¤§è¯­è¨€æ¨¡å‹è¦å…·æœ‰å¤æ‚ç†è§£ä»¥åŠç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸æ˜¯ä¸ºç‰¹å®šçš„è¯­è¨€ä»»åŠ¡æ‰€è®¾è®¡è€Œæ˜¯å…·æœ‰æ›´å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹çš„æˆåŠŸå½’å› äºTransformeræ¶æ„ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„æµ·é‡æ•°æ®ã€‚
é€šè¿‡ç¼–å†™ä»£ç åŸºäºTransformeræ¶æ„å®ç°ç±»ä¼¼ChatGPTçš„å¤§è¯­è¨€æ¨¡å‹ã€‚
å¤§è¯­è¨€æ¨¡å‹çš„å¤§æ—¢æŒ‡å‚æ•°é‡è§„æ¨¡ï¼ŒåˆæŒ‡æµ·é‡æ•°æ®ã€‚
Transformerå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯å®ƒèƒ½å¤Ÿé€‰æ‹©æ€§çš„å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚å…³é”®ç»„ä»¶æ˜¯self-attentionï¼Œèƒ½å¤Ÿè¡¡é‡è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªtokenç›¸å¯¹äºå…¶ä»–tokençš„ç›¸å¯¹é‡è¦æ€§ã€‚
å¤§è¯­è¨€æ¨¡å‹çš„æ„å»ºåŒ…å«pretraining and fine-tuningä¸¤ä¸ªé˜¶æ®µã€‚pretrainingé˜¶æ®µæ˜¯åœ¨æµ·é‡unlabeled text dataä¸Šè¿›è¡Œself-supervised learningï¼Œå»å­¦åˆ°ä¸€äº›general representionã€‚fine-tuningæ˜¯train on labeled dataã€‚
Transformeråˆ†ä¸ºencoderå’Œdecoderéƒ¨åˆ†ã€‚encoderè´Ÿè´£å°†è¾“å…¥åºåˆ—ç¼–ç æˆa series of numerical representations or vectors that capture the contextual information of the inputï¼Œè§£ç å™¨æ ¹æ®å½“å‰çš„these encoded vectorsä»¥åŠå½“å‰è¾“å…¥å®Œæˆä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ã€‚
å®ç°é¢„è®­ç»ƒçš„ä»£ç ã€å¤ç”¨å…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹
The next-word prediction taskæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦ä¸ºè®­ç»ƒæ•°æ®æä¾›æ ‡ç­¾ï¼Œä»–åˆ©ç”¨æ•°æ®è‡ªèº«çš„ç»“æ„ã€‚ç”¨æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªè¯ä½œä¸ºè¦è®­ç»ƒçš„æ ‡ç­¾ã€‚
Autoregressive modelsæ•´åˆæ—©å…ˆçš„è¾“å‡ºä½œä¸ºæœªæ¥é¢„æµ‹çš„è¾“å…¥ã€‚
original transformer çš„encoderå’Œdecoderé‡å¤6æ¬¡ã€‚GPT-3æœ‰96å±‚transformer layers ä»¥åŠ175bçš„parametersã€‚
emergent behaviorï¼šæ˜¯æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæœªè¢«æ˜¾ç¤ºè®­ç»ƒçš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚

Working with text data
ğŸthe required steps for preparing the embeddings used by an LLM
splitting text into individual word and subword tokens"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tyh382596868.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Transformer","item":"https://tyh382596868.github.io/posts/transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer","name":"Transformer","description":"Transformer å¤§è¯­è¨€æ¨¡å‹è¦å…·æœ‰å¤æ‚ç†è§£ä»¥åŠç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸æ˜¯ä¸ºç‰¹å®šçš„è¯­è¨€ä»»åŠ¡æ‰€è®¾è®¡è€Œæ˜¯å…·æœ‰æ›´å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹çš„æˆåŠŸå½’å› äºTransformeræ¶æ„ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„æµ·é‡æ•°æ®ã€‚\né€šè¿‡ç¼–å†™ä»£ç åŸºäºTransformeræ¶æ„å®ç°ç±»ä¼¼ChatGPTçš„å¤§è¯­è¨€æ¨¡å‹ã€‚\nå¤§è¯­è¨€æ¨¡å‹çš„å¤§æ—¢æŒ‡å‚æ•°é‡è§„æ¨¡ï¼ŒåˆæŒ‡æµ·é‡æ•°æ®ã€‚\nTransformerå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯å®ƒèƒ½å¤Ÿé€‰æ‹©æ€§çš„å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚å…³é”®ç»„ä»¶æ˜¯self-attentionï¼Œèƒ½å¤Ÿè¡¡é‡è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªtokenç›¸å¯¹äºå…¶ä»–tokençš„ç›¸å¯¹é‡è¦æ€§ã€‚\nå¤§è¯­è¨€æ¨¡å‹çš„æ„å»ºåŒ…å«pretraining and fine-tuningä¸¤ä¸ªé˜¶æ®µã€‚pretrainingé˜¶æ®µæ˜¯åœ¨æµ·é‡unlabeled text dataä¸Šè¿›è¡Œself-supervised learningï¼Œå»å­¦åˆ°ä¸€äº›general representionã€‚fine-tuningæ˜¯train on labeled dataã€‚\nTransformeråˆ†ä¸ºencoderå’Œdecoderéƒ¨åˆ†ã€‚encoderè´Ÿè´£å°†è¾“å…¥åºåˆ—ç¼–ç æˆa series of numerical representations or vectors that capture the contextual information of the inputï¼Œè§£ç å™¨æ ¹æ®å½“å‰çš„these encoded vectorsä»¥åŠå½“å‰è¾“å…¥å®Œæˆä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ã€‚\nå®ç°é¢„è®­ç»ƒçš„ä»£ç ã€å¤ç”¨å…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹\nThe next-word prediction taskæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦ä¸ºè®­ç»ƒæ•°æ®æä¾›æ ‡ç­¾ï¼Œä»–åˆ©ç”¨æ•°æ®è‡ªèº«çš„ç»“æ„ã€‚ç”¨æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªè¯ä½œä¸ºè¦è®­ç»ƒçš„æ ‡ç­¾ã€‚\nAutoregressive modelsæ•´åˆæ—©å…ˆçš„è¾“å‡ºä½œä¸ºæœªæ¥é¢„æµ‹çš„è¾“å…¥ã€‚\noriginal transformer çš„encoderå’Œdecoderé‡å¤6æ¬¡ã€‚GPT-3æœ‰96å±‚transformer layers ä»¥åŠ175bçš„parametersã€‚\nemergent behaviorï¼šæ˜¯æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæœªè¢«æ˜¾ç¤ºè®­ç»ƒçš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚\nWorking with text data ğŸthe required steps for preparing the embeddings used by an LLM\nsplitting text into individual word and subword tokens\n","keywords":[],"articleBody":"Transformer å¤§è¯­è¨€æ¨¡å‹è¦å…·æœ‰å¤æ‚ç†è§£ä»¥åŠç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸æ˜¯ä¸ºç‰¹å®šçš„è¯­è¨€ä»»åŠ¡æ‰€è®¾è®¡è€Œæ˜¯å…·æœ‰æ›´å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹çš„æˆåŠŸå½’å› äºTransformeræ¶æ„ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„æµ·é‡æ•°æ®ã€‚\né€šè¿‡ç¼–å†™ä»£ç åŸºäºTransformeræ¶æ„å®ç°ç±»ä¼¼ChatGPTçš„å¤§è¯­è¨€æ¨¡å‹ã€‚\nå¤§è¯­è¨€æ¨¡å‹çš„å¤§æ—¢æŒ‡å‚æ•°é‡è§„æ¨¡ï¼ŒåˆæŒ‡æµ·é‡æ•°æ®ã€‚\nTransformerå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯å®ƒèƒ½å¤Ÿé€‰æ‹©æ€§çš„å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚å…³é”®ç»„ä»¶æ˜¯self-attentionï¼Œèƒ½å¤Ÿè¡¡é‡è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªtokenç›¸å¯¹äºå…¶ä»–tokençš„ç›¸å¯¹é‡è¦æ€§ã€‚\nå¤§è¯­è¨€æ¨¡å‹çš„æ„å»ºåŒ…å«pretraining and fine-tuningä¸¤ä¸ªé˜¶æ®µã€‚pretrainingé˜¶æ®µæ˜¯åœ¨æµ·é‡unlabeled text dataä¸Šè¿›è¡Œself-supervised learningï¼Œå»å­¦åˆ°ä¸€äº›general representionã€‚fine-tuningæ˜¯train on labeled dataã€‚\nTransformeråˆ†ä¸ºencoderå’Œdecoderéƒ¨åˆ†ã€‚encoderè´Ÿè´£å°†è¾“å…¥åºåˆ—ç¼–ç æˆa series of numerical representations or vectors that capture the contextual information of the inputï¼Œè§£ç å™¨æ ¹æ®å½“å‰çš„these encoded vectorsä»¥åŠå½“å‰è¾“å…¥å®Œæˆä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ã€‚\nå®ç°é¢„è®­ç»ƒçš„ä»£ç ã€å¤ç”¨å…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹\nThe next-word prediction taskæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦ä¸ºè®­ç»ƒæ•°æ®æä¾›æ ‡ç­¾ï¼Œä»–åˆ©ç”¨æ•°æ®è‡ªèº«çš„ç»“æ„ã€‚ç”¨æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªè¯ä½œä¸ºè¦è®­ç»ƒçš„æ ‡ç­¾ã€‚\nAutoregressive modelsæ•´åˆæ—©å…ˆçš„è¾“å‡ºä½œä¸ºæœªæ¥é¢„æµ‹çš„è¾“å…¥ã€‚\noriginal transformer çš„encoderå’Œdecoderé‡å¤6æ¬¡ã€‚GPT-3æœ‰96å±‚transformer layers ä»¥åŠ175bçš„parametersã€‚\nemergent behaviorï¼šæ˜¯æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæœªè¢«æ˜¾ç¤ºè®­ç»ƒçš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚\nWorking with text data ğŸthe required steps for preparing the embeddings used by an LLM\nsplitting text into individual word and subword tokens\nconverting words into tokens\nturning tokens into embedding vectors\nğŸbe encoded into vector representations\nğŸadvanced tokenization schemes like byte pair encoding\nğŸimplement a sampling and data-loading strategy to produce the input-output pairs\n2.1 Understanding word embeddings\nembeddingï¼šå°†data convertæˆvector representionã€‚\nan embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector spaceã€‚\nå› æ­¤éœ€è¦represent words as continuous-valued vectorsã€‚\ntext embedingåŒ…å«word embedingã€embeddings for sentences, paragraphs, or whole documentsã€‚\nSentence or paragraph embeddings are popular choices for retrieval-augmented generationã€‚\nWord2Vecæ€æƒ³æ˜¯ç›¸ä¼¼ä¸Šä¸‹æ–‡é‡Œçš„å•è¯å…·æœ‰ç›¸ä¼¼çš„è¯­ä¹‰ï¼ŒæŠ•å½±åˆ°å‘é‡ç©ºé—´æ—¶clustered togetherã€‚\nLLMä¼šè‡ªå·±ç”ŸæˆåµŒå…¥å‘é‡è€Œä¸æ˜¯ç”¨pretrained models such as Word2Vecã€‚\nThe smallest GPT-2 models (117M and 125M parameters)ï¼šan embedding size of 768 dimensionsã€‚\nThe largest GPT-3 model (175B parameters)ï¼šan embedding size of 12,288 dimensions\n2.2 Tokenizing text\nspliting input text into individual tokens\n# use the re,split command with the following syntax to split a text on whitespace charaters import re text = \"Hello, world. This, is a test.\" result = re.split(r'(\\s)', text) print(result) The result is a list of individual words,whitespaces,and punctuation characters\n['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n# modify the regular expression splits on whitespaces (\\s), commas, and periods ([,.]) result = re.split(r'([,.]|\\s)', text) print(result) the words and punctuation characters are now separate list entries\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is',' ', 'a', ' ', 'test', '.', '']\n# remove these redundant characters # strip() æ˜¯å­—ç¬¦ä¸²ï¼ˆstrï¼‰å¯¹è±¡çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨æ¥å»æ‰å­—ç¬¦ä¸²ä¸¤ç«¯çš„æŒ‡å®šå­—ç¬¦ï¼ˆé»˜è®¤æ˜¯ç©ºç™½ç¬¦ï¼ŒåŒ…æ‹¬ç©ºæ ¼ã€æ¢è¡Œç¬¦\\nã€åˆ¶è¡¨ç¬¦\\t ç­‰ï¼‰ result = [item for item in result if item.strip()] print(result) whitespace-free output\n['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\nRemoving whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example,Python code, which is sensitive to indentation and spacing).\n# handle other types of punctuation, such as question marks, quotation marks, and the double-dashes text = \"Hello, world. Is this-- a test?\" result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) result = [item.strip() for item in result if item.strip()] print(result) ['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n2.3 Converting tokens into token IDs\nconvert these tokens from a Python string to an integer representation to produce the token IDsã€‚\nbuild a vocabularyã€‚This vocabulary defines how we map each unique word and special character to a unique integerã€‚\n# create a list of all unique tokens and sort them alphabetically all_words = sorted(set(preprocessed)) # create the vocabulary which defines how we map each unique word and special character to a unique integer vocab = {token:integer for integer,token in enumerate(all_words)} apply this vocabulary to convert new text into token IDs and turn token IDs into textã€‚\n# implement a complete tokenizer class # with an encode method that splits text into tokens # and carries out the string-to-integer mapping to produce tokenIDs via the vocabulary # a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text. class SimpleTokenizerV1: def __init__(self, vocab): self.str_to_int = vocab self.int_to_str = {i:s for s,i in vocab.items()} # .items() return all key-value pairs of dict.such as dict_items([('hello', 1), ('world', 2)]) def encode(self, text): preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text) preprocessed = [ item.strip() for item in preprocessed if item.strip()] # ifåçš„stripæ˜¯åˆ¤æ–­æœ‰æ²¡æœ‰ç©ºå†…å®¹ï¼Œæ¯”å¦‚å…¨ç©ºæ ¼æˆ–è€…æ¢è¡Œç¬¦ï¼Œåˆ¤æ–­falseè¿‡æ»¤æ‰ # æœ€å‰é¢çš„stripæ˜¯å¦‚æœä¸æ˜¯å…¨ç©ºå†…å®¹ï¼Œå°±å¯¹å†…å®¹è¿›è¡Œæ¸…æ´—å»æ‰ç©ºæ ¼æ¢è¡Œç¬¦ä¿ç•™æ¸¸æ³³å†…å®¹ # æ²¡æœ‰ifåçš„stripå‰é¢çš„stripèƒ½ä¸èƒ½æ¸…æ´—æ‰å…¨ç©ºçš„å†…å®¹,ä¸è¡Œã€‚â€œ â€.strip()ä¼šå˜æˆâ€œâ€ ids = [self.str_to_int[s] for s in preprocessed] return ids def decode(self, ids): text = \" \".join([self.int_to_str[i] for i in ids]) text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # åœ¨å­—ç¬¦ä¸² string ä¸­ï¼ŒæŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆ pattern çš„å­ä¸²ï¼Œå¹¶ç”¨ repl æ›¿æ¢æ‰ã€‚ # åœ¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ï¼ˆre.subï¼‰é‡Œï¼Œ\\1 è¡¨ç¤ºï¼šå¼•ç”¨ç¬¬ 1 ä¸ªæ‹¬å·é‡Œæ•è·çš„å†…å®¹ã€‚ # \\s+ â†’ ä¸€ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ï¼‰ # ([,.?!\"()']) â†’ æ•è·æ‹¬å·å†…çš„ä»»æ„ä¸€ä¸ªæ ‡ç‚¹ç¬¦å· # \\1 â†’ æ­£åˆ™ä¸­ç¬¬ä¸€ä¸ªæ‹¬å·æ•è·çš„å†…å®¹ï¼ˆå³æ ‡ç‚¹ç¬¦å·æœ¬èº«ï¼‰ # ä½œç”¨å°±æ˜¯ï¼šå»æ‰æ ‡ç‚¹å‰é¢çš„å¤šä½™ç©ºæ ¼ã€‚ return text \"\"\" In [18]: a = ['Hello', 'world', ' ! ', ' '] In [19]: [item.strip() for item in a] Out[19]: ['Hello', 'world', '!', ''] In [20]: [item.strip() for item in a if item.strip()] Out[20]: ['Hello', 'world', '!'] In [21]: [item for item in a if item.strip()] Out[21]: ['Hello', 'world', ' ! '] \"\"\" Using the SimpleTokenizerV1 Python class, we can now instantiate new tokenizer objects via an existing vocabulary, which we can then use to encode and decode text\ntokenizer = SimpleTokenizerV1(vocab) text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\" ids = tokenizer.encode(text) print(ids) print(tokenizer.decode(ids)) 2.4 Adding special context tokens\nmodify the tokenizer to handle unknown words and address the usage and addition of special context tokensã€‚\nspecial tokens including markers for unknown words and document boundaries, \u003c|unk|\u003e and \u003c|endoftext|\u003e\n# add and \u003c|endoftext|\u003e to list of all unique words. all_tokens = sorted(list(set(preprocessed))) all_tokens.extend([\"\u003c|endoftext|\u003e\", \"\u003c|unk|\u003e\"]) vocab = {token:integer for integer,token in enumerate(all_tokens)} A simple text tokenizer that handles unknown words\nclass SimpleTokenizerV2: def __init__(self, vocab): self.str_to_int = vocab self.int_to_str = { i:s for s,i in vocab.items()} def encode(self, text): preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) preprocessed = [ item.strip() for item in preprocessed if item.strip() ] preprocessed = [item if item in self.str_to_int else \"\u003c|unk|\u003e\" for item in preprocessed] ids = [self.str_to_int[s] for s in preprocessed] return ids def decode(self, ids): text = \" \".join([self.int_to_str[i] for i in ids]) text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text) return text 2.5 Byte pair encoding\nPython open source library called tiktoken (https://github.com/openai/tiktoken), which implements the BPE algorithm very efficiently based on source code in Rust.\npip install tiktoken\nThe code we will use is based on tiktoken 0.7.0.\nfrom importlib.metadata import version import tiktoken print(\"tiktoken version:\", version(\"tiktoken\")) # instantiate the BPE tokenizer from tiktoken tokenizer = tiktoken.get_encoding(\"gpt2\") # an encode method: text = ( \"Hello, do you like tea? \u003c|endoftext|\u003e In the sunlit terraces\" \"of someunknownPlace.\" ) integers = tokenizer.encode(text, allowed_special={\"\u003c|endoftext|\u003e\"}) print(integers) # the decode method strings = tokenizer.decode(integers) print(strings) the BPE tokenizer has a total vocabulary size of 50,257\nBPE breaks down words that arenâ€™t in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words.\n2.6 Data sampling with a sliding window\n2.7 Creating token embeddings\nconvert the token IDs into embedding vectors\nhow the token ID to embedding vector conversion\n# the embedding layer is essentially a lookup operation # that retrieves rows from the embedding layerâ€™s weight matrix via a token ID. torch.manual_seed(123) embedding_layer = torch.nn.Embedding(vocab_size, output_dim) print(embedding_layer.weight) # apply embedding layer to a token ID to obtain the embedding vector print(embedding_layer(torch.tensor([3]))) print(embedding_layer(torch.tensor([2, 3, 5, 1]))) # Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix 2.8 Encoding word positions\nit is helpful to inject additional position information into the LLM\ntwo broad categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings\nOpenAIâ€™s GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original transformer model.\n# token embedding vocab_size = 50257 output_dim = 256 token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # position embedding context_length = max_length pos_embedding_layer = torch.nn.Embedding(context_length, output_dim) pos_embeddings = pos_embedding_layer(torch.arange(context_length)) # add pos_embeddings to token_embeddings input_embeddings = token_embeddings + pos_embeddings Coding attention mechanisms ğŸsimplified attention mechanism\nğŸadd a causal attention mask to prevent the LLM from accessing future tokens\nğŸadd a dropout mask to reduce overfitting in LLMs\nğŸmulti-head attention: multiple instances of causal attention\nğŸcreating multi-head attention modules involves batched matrix multiplications\n3.4.2 Implementing a compact self-attention Python class\ninitializes trainable weight matrices (W_query, W_key, and W_value) compute the attention scores (attn_scores) by multiplying queries and keys normalizing these scores using softmax to get attn_weights create a context vector by weighting the values with these attn_weights $\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\na significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(â€¦)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.\nclass SelfAttention_v2(nn.Module): def __init__(self, d_in, d_out, qkv_bias=False): super().__init__() self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) def forward(self, x): # shape of x: (seq_len, d_in) keys = self.W_key(x) # shape of keys: (seq_len, d_out) queries = self.W_query(x) # shape of keys: (seq_len, d_out) values = self.W_value(x) # shape of keys: (seq_len, d_out) attn_scores = queries @ keys.T # (seq_len, seq_len) attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1 ) # keys.shape[-1]: d_out # torch.softmax(......, dim=-1),æŒ‰æœ€åä¸€ç»´ç®—softmaxï¼Œä¹Ÿå°±æ˜¯æŒ‰è¡Œ context_vec = attn_weights @ values return context_vec # inputs contains six embedding vectors # results contains six context vectors\ttorch.manual_seed(123) sa_v1 = SelfAttention_v1(d_in, d_out) print(sa_v1(inputs)) 3.5 Hiding future words with causal attention\nCausal attention, also known as masked attention è®©æ¨¡å‹åªå…³æ³¨sequenceä¸­çš„previous å’Œ current input\nmask out the future tokens\ndropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors.\napply the dropout mask after computing the attention weightsæ›´å¸¸è§ã€‚\nclass CausalAttention(nn.Module): def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False): super().__init__() self.d_out = d_out self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) self.dropout = nn.Dropout(dropout) self.register_buffer( 'mask', torch.triu(torch.ones(context_length, context_length), diagonal=1) ) def forward(self, x): b, num_tokens, d_in = x.shape keys = self.W_key(x) queries = self.W_query(x) values = self.W_value(x) attn_scores = queries @ keys.transpose(1, 2) # creating a mask with 1s above the diagonal # and then replacing these 1s with negative infinity (-inf) values attn_scores.masked_fill_( self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1 ) attn_weights = self.dropout(attn_weights) context_vec = attn_weights @ values return context_vec torch.manual_seed(123) context_length = batch.shape[1] ca = CausalAttention(d_in, d_out, context_length, 0.0) context_vecs = ca(batch) print(\"context_vecs.shape:\", context_vecs.shape) 3.6 Extending single-head attention to multi-head attention\nprocessing the heads in sequential\nmultiple heads are implemented by creating a list of CausalAttention objects (self.heads)\nclass MultiHeadAttentionWrapper(nn.Module): def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False): super().__init__() self.heads = nn.ModuleList( [CausalAttention( d_in, d_out, context_length, dropout, qkv_bias ) for _ in range(num_heads)] ) def forward(self, x): return torch.cat([head(x) for head in self.heads], dim=-1) processing the heads in parallel\nsplits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention\nsplit the d_out dimension into num_heads and head_dim, where head_dim = d_out / num_heads.\nThis splitting is then achieved using the .view method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens, num_heads, head_dim)\nclass MultiHeadAttention(nn.Module): def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False): super().__init__() assert (d_out % num_heads == 0), \\ \"d_out must be divisible by num_heads\" self.d_out = d_out self.num_heads = num_heads self.head_dim = d_out // num_heads self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) self.out_proj = nn.Linear(d_out, d_out) self.dropout = nn.Dropout(dropout) self.register_buffer( \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1) ) def forward(self, x): b, num_tokens, d_in = x.shape keys = self.W_key(x) queries = self.W_query(x) values = self.W_value(x) keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) values = values.view(b, num_tokens, self.num_heads, self.head_dim) queries = queries.view( b, num_tokens, self.num_heads, self.head_dim ) keys = keys.transpose(1, 2) queries = queries.transpose(1, 2) values = values.transpose(1, 2) attn_scores = queries @ keys.transpose(2, 3) mask_bool = self.mask.bool()[:num_tokens, :num_tokens] attn_scores.masked_fill_(mask_bool, -torch.inf) attn_weights = torch.softmax( attn_scores / keys.shape[-1]**0.5, dim=-1) attn_weights = self.dropout(attn_weights) context_vec = (attn_weights @ values).transpose(1, 2) context_vec = context_vec.contiguous().view( b, num_tokens, self.d_out ) context_vec = self.out_proj(context_vec) return context_vec torch.manual_seed(123) batch_size, context_length, d_in = batch.shape d_out = 2 mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2) context_vecs = mha(batch) print(context_vecs) print(\"context_vecs.shape:\", context_vecs.shape) Implementing a GPT model from scratch to generate text This chapter covers\nCoding a GPT-like large language model (LLM) that can be trained to generate human-like text Normalizing layer activations to stabilize neural network training Adding shortcut connections in deep neural networks Implementing transformer blocks to create GPT models of various sizes Computing the number of parameters and storage requirements of GPT models 4.1 Coding an LLM architecture\nparametersæŒ‡çš„æ˜¯trainable weights of the model\na GPT placeholder architecture (DummyGPTModel)\nthe order in which we tackle the individual concepts required to code the final GPT architecture\nå…ˆcodeå‡ºä¸€ä¸ªGPT placeholder architecture calling DummyGPTModelï¼Œç„¶åå¾—åˆ°the individual core pieces ï¼Œæœ€ç»ˆassemblingèµ·æ¥ã€‚\nä¸€ä¸ªDummyGPTModelåŒ…æ‹¬token embeddings , positional embedding , dropout , ä¸€ç³»åˆ—çš„transformer blocks(DummyTransformerBlock) æœ€åä¸€ä¸ªLayer Normalization(DummyLayerNorm)å’Œä¸€ä¸ªLinear output layer\nGPT_CONFIG_124M = { \"vocab_size\": 50257, # Vocabulary size \"context_length\": 1024, # modelèƒ½å¤Ÿå¤„ç†çš„æœ€å¤§tokenå’Œpositional embeddingæ•°é‡ \"emb_dim\": 768, # æ¯ä¸ªTokençš„Embedding size \"n_heads\": 12, # Number of attention heads \"n_layers\": 12, # Transformer Blockçš„æ•°é‡ \"drop_rate\": 0.1, # Dropout rate \"qkv_bias\": False # Query-Key-Value bias } # A placeholder GPT model architecture class import torch import torch.nn as nn class DummyGPTModel(nn.Module): def __init__(self, cfg): super().__init__() self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]) self.trf_blocks = nn.Sequential( *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])] ) # *èµ·unpackçš„ä½œç”¨ï¼Œå…·ä½“æ˜¯å°†åˆ—è¡¨é‡Œçš„å…ƒç´ ç»™ä¸€ä¸€æ‹¿å‡ºæ¥ã€‚ self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]) self.out_head = nn.Linear( cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False ) def forward(self, in_idx): batch_size, seq_len = in_idx.shape tok_embeds = self.tok_emb(in_idx) pos_embeds = self.pos_emb( torch.arange(seq_len, device=in_idx.device) ) x = tok_embeds + pos_embeds x = self.drop_emb(x) x = self.trf_blocks(x) x = self.final_norm(x) logits = self.out_head(x) return logits class DummyTransformerBlock(nn.Module): def __init__(self, cfg): super().__init__() def forward(self, x): return x class DummyLayerNorm(nn.Module): def __init__(self, normalized_shape, eps=1e-5): super().__init__() def forward(self, x): return x 4.2 Normalizing activations with layer normalization\nä¸ºä»€ä¹ˆç”¨batch normalizationè€Œä¸æ˜¯layer normalizationï¼Ÿ\nè®­ç»ƒæœ‰è®¸å¤šlayersçš„deep neural networkæ—¶ä¼šé‡åˆ°vanishing or exploding gradientsçš„é—®é¢˜ã€‚\nå®ç°Layer normalizationå¯ä»¥æå‡è®­ç»ƒçš„stabilityå’Œefficiencyï¼Œ\nlayer normalizationä¸€èˆ¬æ˜¯æ”¾åœ¨multi-head attention moduleçš„å‰å\ndim=-1è¡¨ç¤ºå‘é‡çš„æœ€åä¸€ç»´ï¼Œå¯¹äºä¸€ä¸ªtwo dimensional tensoræ¥è¯´ä¹Ÿå°±æ˜¯å‘é‡çš„columnsã€‚å¯¹äºä¸€ä¸ªä¸‰ç»´å‘é‡[bs, seq_len, embedding_size]æ¥è¯´æœ€åä¸€ç»´å°±æ˜¯æ¯ä¸ªtokençš„embeddin\n_size.\nlayer normalizationçš„æ“ä½œï¼šout_norm = (out - mean) / torch.sqrt(var).\nlayer normalizationä¸€èˆ¬æ˜¯åœ¨è¾“å…¥tensorçš„last dimensionæ“ä½œçš„ï¼Œè¿™ä»£è¡¨embedding dimension (emb_dim)ã€‚\nclass LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.scale = nn.Parameter(torch.ones(emb_dim)) self.shift = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) # é‡‡ç”¨Biased varianceæœ‰åä¼°è®¡æ¥æ±‚æ ·æœ¬æ–¹å·®ï¼Œå› ä¸ºç»´åº¦768è¶³å¤Ÿå¤§ï¼ŒåŒºåˆ«ä¸å¤§ var = x.var(dim=-1, keepdim=True, unbiased=False) norm_x = (x - mean) / torch.sqrt(var + self.eps) # epsæ˜¯æå°å€¼ï¼Œé˜²æ­¢division by zeroçš„æƒ…å†µ return self.scale * norm_x + self.shift # scale å’Œ shiftæ˜¯ä¸¤ä¸ªtrainable parameters,åˆå§‹åˆ†åˆ«æ˜¯1ï¼Œ0ï¼Œ # è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚æœè¢«åˆ¤å®šè°ƒæ•´ä»–èƒ½æå‡performanceï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨çš„è°ƒæ•´ä»–ã€‚ æ–¹å·®ä¸æ ·æœ¬æ–¹å·®ï¼Œä»¥åŠæ ·æœ¬æ–¹å·®çš„æœ‰åä¼°è®¡ä¸æ— åä¼°è®¡ã€‚\næ–¹å·®æ˜¯é™¤nï¼Œä¸ºä»€ä¹ˆæ ·æœ¬æ–¹å·®é™¤nåè€Œæ˜¯æœ‰åä¼°è®¡ï¼Œæ ·æœ¬æ–¹å·®æ— åä¼°è®¡æ˜¯é™¤ä»¥n-1ï¼Ÿå› ä¸ºé‡‡æ ·è¿‡ç¨‹ä¸­ä¼šä½ä¼°æ–¹å·®ï¼Œæ‰€ä»¥è¦é€šè¿‡è´å¡å°”ä¿®æ­£æ¥ä¿®æ­£æ–¹å·®ã€‚æ–¹å·®æ˜¯ååº”æ•°æ®ç¦»æ•£ç¨‹åº¦çš„ï¼Œä»æ‰€æœ‰çš„æ•°æ®ä¸­é‡‡æ ·ä¸€äº›æ ·æœ¬ï¼Œç”¨æ ·æœ¬çš„æ ·æœ¬æ–¹å·®æ¥ååº”æ€»ä½“æ–¹å·®çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯é‡‡æ ·æ ·æœ¬çš„è¿‡ç¨‹è‚¯å®šæ˜¯æ¦‚ç‡è¶Šå¤§çš„è¶Šå®¹æ˜“é‡‡æ ·ï¼Œå°±å¯¼è‡´é‡‡æ ·çš„æ ·æœ¬ç¦»æ•£ç¨‹åº¦æ›´é›†ä¸­ï¼Œå½“ç„¶å¦‚æœé‡‡æ ·çš„æ•°ç›®nè¶³å¤Ÿå¤§ï¼Œé‡‡æ ·çš„æ ·æœ¬çš„åˆ†å¸ƒå°±æ— é™åˆ†å¸ƒæ€»ä½“çš„åˆ†å¸ƒäº†ã€‚ä»¥å¦‚ä¸‹æ­£å¤ªåˆ†å¸ƒä¸ºä¾‹ã€‚\n4.3 Implementing a feed forward network with GELU activations\nä¸ºä»€ä¹ˆç”¨GELUï¼ˆé¸¡è·¯ï¼‰æ¿€æ´»å‡½æ•°è€Œä¸æ˜¯reluæ¿€æ´»å‡½æ•°ï¼Ÿå› ä¸ºreluæ¿€æ´»å‡½æ•°åœ¨0çš„åœ°æ–¹ä¸å¯å¾®ã€‚\ndead neuronsï¼šå½“è¾“å…¥å°äº0æ—¶è¾“å‡ºæ°¸è¿œæ˜¯0å¯¹å­¦ä¹ æ²¡æœ‰ä»€ä¹ˆè´¡çŒ®ï¼Œæ‰€ä»¥å«dead neuronsã€‚\nGELU(x) = xâ‹…Î¦(x), where Î¦(x) is the cumulative distribution function of the standard Gaussian distributionã€‚\nstandard Gaussian distribution\nstandard Gaussian distributionçš„CDF\n(the original GPT-2 model was also trained with this approximation, which was found via curve fittingï¼‰\nm = nn.GELU()\nAn implementation of the GELU activation function\nclass GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) A feed forward neural network module\nclass FeedForward(nn.Module): def __init__(self, cfg): super().__init__() self.layers = nn.Sequential( nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), GELU(), nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ) def forward(self, x): return self.layers(x) 4.4 Adding shortcut connections\nshort connectionså’Œresidual connectionæ˜¯ä¸€å›äº‹ï¼Œç”¨æ¥è§£å†³vanishing gredient problemã€‚\nvanishing gradient problem æ˜¯æŒ‡åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦è¿‡å°å¯¼è‡´å­¦ä¹ åœæ»ä¸å‰ï¼Œconvergence delayã€‚\nä¸åŠ Skip Connection åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦ä¼šè¶Šæ¥è¶Šå°ã€‚skip connectionå¯ä»¥ä¸ºæ¢¯åº¦åˆ›å»ºä¸€ä¸ªå¦å¤–çš„æ›´çŸ­çš„è·¯å¾„ï¼Œé€šè¿‡skipping one or more layersè®©gradient flow\nVisualizing the Loss Landscape of Neural Nets\nä¸åŠ Skip Connectionæœ‰å¾ˆå¤šçš„å±€éƒ¨æœ€å°å€¼ï¼Œä¼šå¯¼è‡´ä¼˜åŒ–å›°éš¾\nclass ExampleDeepNeuralNetwork(nn.Module): def __init__(self, layer_sizes, use_shortcut): super().__init__() self.use_shortcut = use_shortcut self.layers = nn.ModuleList([ nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()), nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()), nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()), nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()), nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()) ]) def forward(self, x): for layer in self.layers: layer_output = layer(x) if self.use_shortcut and x.shape == layer_output.shape: x = x + layer_output else: x = layer_output return x a function that computes the gradients\ndef print_gradients(model, x): output = model(x) target = torch.tensor([[0.]]) loss = nn.MSELoss() loss = loss(output, target) loss.backward() for name, param in model.named_parameters(): if 'weight' in name: print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\") 4.5 Connecting attention and linear layers in a transformer block\nimplement the transformer block åŒ…æ‹¬ multi-head attention, layer normalization, dropout, feed forward layers, and GELU activationsã€‚\nTransformerBlockçš„æ ¸å¿ƒæ˜¯åŒ…æ‹¬ a multi-head attention mechanism (MultiHeadAttention) and a feed forward network (FeedForward)ã€‚å…¶ä¸­Layer normalization (LayerNorm)æ˜¯åœ¨ä»¥ä¸Šä¸¤éƒ¨åˆ†çš„å‰é¢ï¼Œdropoutæ˜¯åœ¨ä»¥ä¸Šä¸¤éƒ¨åˆ†çš„åé¢ã€‚layer normalizationåœ¨å‰é¢æ˜¯å«Pre-LayerNormï¼Œåœ¨åŸè®ºæ–‡ä¸­LayerNormæ˜¯åœ¨åé¢å«Post-LayerNorm\nclass TransformerBlock(nn.Module): def __init__(self, cfg): super().__init__() self.att = MultiHeadAttention( d_in=cfg[\"emb_dim\"], d_out=cfg[\"emb_dim\"], context_length=cfg[\"context_length\"], num_heads=cfg[\"n_heads\"], dropout=cfg[\"drop_rate\"], qkv_bias=cfg[\"qkv_bias\"]) self.ff = FeedForward(cfg) self.norm1 = LayerNorm(cfg[\"emb_dim\"]) self.norm2 = LayerNorm(cfg[\"emb_dim\"]) self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"]) def forward(self, x): shortcut = x x = self.norm1(x) x = self.att(x) x = self.drop_shortcut(x) x = x + shortcut shortcut = x x = self.norm2(x) x = self.ff(x) x = self.drop_shortcut(x) x = x + shortcut return x 4.6 Coding the GPT model\nclass GPTModel(nn.Module): def __init__(self, cfg): super().__init__() self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]) self.trf_blocks = nn.Sequential( *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) self.final_norm = LayerNorm(cfg[\"emb_dim\"]) self.out_head = nn.Linear( cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False ) def forward(self, in_idx): batch_size, seq_len = in_idx.shape tok_embeds = self.tok_emb(in_idx) pos_embeds = self.pos_emb( torch.arange(seq_len, device=in_idx.device) ) x = tok_embeds + pos_embeds x = self.drop_emb(x) x = self.trf_blocks(x) x = self.final_norm(x) logits = self.out_head(x) return logits # è®¡ç®—modelçš„parametersæ•°é‡ total_params = sum(p.numel() for p in model.parameters()) print(f\"Total number of parameters: {total_params:,}\") # è®¡ç®—modelçš„parameterså çš„å†…å­˜å¤§å° # Calculates the total size in bytes (assuming float32, 4 bytes per parameter) total_size_bytes = total_params * 4 # Converts to megabytes total_size_mb = total_size_bytes / (1024 * 1024) print(f\"Total size of the model: {total_size_mb:.2f} MB\") ä»¥ä¸Šæ–¹å¼è®¡ç®—çš„æ€»å‚æ•°é‡æ˜¯163,009,536ï¼Œå®é™…GPT-2çš„å‚æ•°é‡æ˜¯124,412,160ã€‚åŸå› æ˜¯GPT-2é‡Œæœ‰ä¸ªå‚æ•°ç»‘å®šï¼ˆweight tyingï¼‰çš„æŠ€æœ¯ã€‚å…·ä½“åšæ³•å°±æ˜¯åœ¨output layeré‡Œå¤ç”¨äº†the token embedding layerçš„æƒé‡ã€‚ä¸ºä»€ä¹ˆè¦è¿™æ ·åšäº†ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªå±‚çš„ç»´åº¦æ˜¯vocabulary size50, 257éå¸¸çš„å·¨å¤§ã€‚\n4.7 Generating text\ngreedy decoding: å–æ¦‚ç‡æœ€å¤§çš„ä½ç½®å¤„çš„tokenã€‚\nç”¨softmax functionå»å°†logitsè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œç”¨torch.argmaxé€‰å‡ºæ¦‚ç‡æœ€å¤§å¤„çš„ç´¢å¼•ã€‚\ndef generate_text_simple(model, idx, max_new_tokens, context_size): ''' max_new_tokens:å¸Œæœ›ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡ã€‚ context_size:æ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ ''' # å¾ªç¯max_new_tokensæ¬¡ï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ–°çš„token for _ in range(max_new_tokens): # å—é™äºæ¨¡å‹èƒ½å¤„ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦context_size,é€‰æœ€æ–°çš„context_sizeä¸ªtokenã€‚ idx_cond = idx[:, -context_size:] with torch.no_grad(): logits = model(idx_cond) # é€‰å‡ºæ¨¡å‹generateçš„token: (bs, n_tokens, vocab_size) ---\u003e (bs, vocab_size) logits = logits[:, -1, :] probas = torch.softmax(logits, dim=-1) # logits ---\u003e Probability distribution idx_next = torch.argmax(probas, dim=-1, keepdim=True) # (bs,1) idx = torch.cat((idx, idx_next), dim=1) Pretraining on unlabeled data 5.1.1 Using GPT to generate text\nimport tiktoken from chapter04 import generate_text_simple def text_to_token_ids(text, tokenizer): encoded = tokenizer.encode(text, allowed_special={'\u003c|endoftext|\u003e'}) encoded_tensor = torch.tensor(encoded).unsqueeze(0) # .unsqueeze(0) adds the batch dimension return encoded_tensor def token_ids_to_text(token_ids, tokenizer): flat = token_ids.squeeze(0) # Removes batch dimension return tokenizer.decode(flat.tolist()) start_context = \"Every effort moves you\" tokenizer = tiktoken.get_encoding(\"gpt2\") token_ids = generate_text_simple( model=model, idx=text_to_token_ids(start_context, tokenizer), max_new_tokens=10, context_size=GPT_CONFIG_124M[\"context_length\"] ) print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer)) 5.1.2 Calculating the text generation loss\nnot just generating next token but also measuring the quality of the generated token\n","wordCount":"2931","inLanguage":"en","image":"https://tyh382596868.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-11-17T12:42:22+08:00","dateModified":"2025-11-17T12:42:22+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tyh382596868.github.io/posts/transformer/"},"publisher":{"@type":"Organization","name":"tyh's blog","logo":{"@type":"ImageObject","url":"https://tyh382596868.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://tyh382596868.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://tyh382596868.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tyh382596868.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://tyh382596868.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://tyh382596868.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://tyh382596868.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Transformer</h1><div class=post-meta><span title='2025-11-17 12:42:22 +0800 CST'>November 17, 2025</span>&nbsp;Â·&nbsp;<span>14 min</span>&nbsp;Â·&nbsp;<span>2931 words</span>&nbsp;Â·&nbsp;<span>Me</span>&nbsp;|&nbsp;<span>
<a href=https://github.com/%3cpath_to_repo%3e/content/posts/transformer.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></span></div></header><div class=post-content><h1 id=transformer>Transformer<a hidden class=anchor aria-hidden=true href=#transformer>#</a></h1><p>å¤§è¯­è¨€æ¨¡å‹è¦å…·æœ‰å¤æ‚ç†è§£ä»¥åŠç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹ä¸æ˜¯ä¸ºç‰¹å®šçš„è¯­è¨€ä»»åŠ¡æ‰€è®¾è®¡è€Œæ˜¯å…·æœ‰æ›´å¹¿æ³›çš„é€šç”¨èƒ½åŠ›ã€‚å¤§è¯­è¨€æ¨¡å‹çš„æˆåŠŸå½’å› äºTransformeræ¶æ„ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„æµ·é‡æ•°æ®ã€‚</p><p>é€šè¿‡ç¼–å†™ä»£ç åŸºäºTransformeræ¶æ„å®ç°ç±»ä¼¼ChatGPTçš„å¤§è¯­è¨€æ¨¡å‹ã€‚</p><p>å¤§è¯­è¨€æ¨¡å‹çš„å¤§æ—¢æŒ‡å‚æ•°é‡è§„æ¨¡ï¼ŒåˆæŒ‡æµ·é‡æ•°æ®ã€‚</p><p>Transformerå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯å®ƒèƒ½å¤Ÿé€‰æ‹©æ€§çš„å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚å…³é”®ç»„ä»¶æ˜¯self-attentionï¼Œèƒ½å¤Ÿè¡¡é‡è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªtokenç›¸å¯¹äºå…¶ä»–tokençš„ç›¸å¯¹é‡è¦æ€§ã€‚</p><p>å¤§è¯­è¨€æ¨¡å‹çš„æ„å»ºåŒ…å«pretraining and fine-tuningä¸¤ä¸ªé˜¶æ®µã€‚pretrainingé˜¶æ®µæ˜¯åœ¨æµ·é‡unlabeled text dataä¸Šè¿›è¡Œself-supervised learningï¼Œå»å­¦åˆ°ä¸€äº›general representionã€‚fine-tuningæ˜¯train on labeled dataã€‚</p><p>Transformeråˆ†ä¸ºencoderå’Œdecoderéƒ¨åˆ†ã€‚encoderè´Ÿè´£å°†è¾“å…¥åºåˆ—ç¼–ç æˆa series of numerical representations or vectors that capture the contextual information of the inputï¼Œè§£ç å™¨æ ¹æ®å½“å‰çš„these encoded vectorsä»¥åŠå½“å‰è¾“å…¥å®Œæˆä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ã€‚</p><p>å®ç°é¢„è®­ç»ƒçš„ä»£ç ã€å¤ç”¨å…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹</p><p>The next-word prediction taskæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦ä¸ºè®­ç»ƒæ•°æ®æä¾›æ ‡ç­¾ï¼Œä»–åˆ©ç”¨æ•°æ®è‡ªèº«çš„ç»“æ„ã€‚ç”¨æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªè¯ä½œä¸ºè¦è®­ç»ƒçš„æ ‡ç­¾ã€‚</p><p>Autoregressive modelsæ•´åˆæ—©å…ˆçš„è¾“å‡ºä½œä¸ºæœªæ¥é¢„æµ‹çš„è¾“å…¥ã€‚</p><p>original transformer çš„encoderå’Œdecoderé‡å¤6æ¬¡ã€‚GPT-3æœ‰96å±‚transformer layers ä»¥åŠ175bçš„parametersã€‚</p><p>emergent behaviorï¼šæ˜¯æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæœªè¢«æ˜¾ç¤ºè®­ç»ƒçš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image.png></p><h3 id=working-with-text-data><em><strong>Working with text data</strong></em><a hidden class=anchor aria-hidden=true href=#working-with-text-data>#</a></h3><p>ğŸthe required steps for preparing the embeddings used by an LLM</p><p>splitting text into individual word and subword tokens</p><p>converting words into tokens</p><p>turning tokens into embedding vectors</p><p>ğŸbe encoded into vector representations</p><p>ğŸadvanced tokenization schemes like byte pair encoding</p><p>ğŸimplement a sampling and data-loading strategy to produce the input-output pairs</p><p><em>2.1 Understanding word embeddings</em></p><p>embeddingï¼šå°†data convertæˆvector representionã€‚</p><p>an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector spaceã€‚</p><p>å› æ­¤éœ€è¦represent words as continuous-valued vectorsã€‚</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%201.png></p><p>text embedingåŒ…å«word embedingã€embeddings for sentences, paragraphs, or whole documentsã€‚</p><p>Sentence or paragraph embeddings are popular choices for <em>retrieval-augmented generationã€‚</em></p><p>Word2Vecæ€æƒ³æ˜¯ç›¸ä¼¼ä¸Šä¸‹æ–‡é‡Œçš„å•è¯å…·æœ‰ç›¸ä¼¼çš„è¯­ä¹‰ï¼ŒæŠ•å½±åˆ°å‘é‡ç©ºé—´æ—¶clustered togetherã€‚</p><p>LLMä¼šè‡ªå·±ç”ŸæˆåµŒå…¥å‘é‡è€Œä¸æ˜¯ç”¨pretrained models such as Word2Vecã€‚</p><p>The smallest GPT-2 models (117M and 125M parameters)ï¼šan embedding size of 768 dimensionsã€‚</p><p>The largest GPT-3 model (175B parameters)ï¼šan embedding size of 12,288 dimensions</p><p>2.2 Tokenizing text</p><p>spliting input text into individual tokens</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%202.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># use the re,split command with the following syntax to split a text on whitespace charaters</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;Hello, world. This, is a test.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;(\s)&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div><p>The result is a list of individual words,whitespaces,and punctuation characters</p><p><code>['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># modify the regular expression splits on whitespaces (\s), commas, and periods ([,.])</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;([,.]|\s)&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div><p>the words and punctuation characters are now separate list entries</p><p><code>['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is',' ', 'a', ' ', 'test', '.', '']</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># remove these redundant characters</span>
</span></span><span class=line><span class=cl><span class=c1># strip() æ˜¯å­—ç¬¦ä¸²ï¼ˆstrï¼‰å¯¹è±¡çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨æ¥å»æ‰å­—ç¬¦ä¸²ä¸¤ç«¯çš„æŒ‡å®šå­—ç¬¦ï¼ˆé»˜è®¤æ˜¯ç©ºç™½ç¬¦ï¼ŒåŒ…æ‹¬ç©ºæ ¼ã€æ¢è¡Œç¬¦\nã€åˆ¶è¡¨ç¬¦\t ç­‰ï¼‰</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=p>[</span><span class=n>item</span> <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>result</span> <span class=k>if</span> <span class=n>item</span><span class=o>.</span><span class=n>strip</span><span class=p>()]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div><p>whitespace-free output</p><p><code>['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']</code></p><p>Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example,Python code, which is sensitive to indentation and spacing).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># handle other types of punctuation, such as question marks, quotation marks, and the double-dashes</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;Hello, world. Is this-- a test?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;([,.:;?_!&#34;()</span><span class=se>\&#39;</span><span class=s1>]|--|\s)&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=p>[</span><span class=n>item</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span> <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>result</span> <span class=k>if</span> <span class=n>item</span><span class=o>.</span><span class=n>strip</span><span class=p>()]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span></code></pre></div><p><code>['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']</code></p><p><em>2.3 Converting tokens into token IDs</em></p><p>convert these tokens from a Python string to an integer representation to produce the token IDsã€‚</p><p>build a vocabularyã€‚This vocabulary defines how we map each unique word and special character to a unique integerã€‚</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%203.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># create a list of all unique tokens and sort them alphabetically</span>
</span></span><span class=line><span class=cl><span class=n>all_words</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=nb>set</span><span class=p>(</span><span class=n>preprocessed</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># create the vocabulary which defines how we map each unique word and special character to a unique integer</span>
</span></span><span class=line><span class=cl><span class=n>vocab</span> <span class=o>=</span> <span class=p>{</span><span class=n>token</span><span class=p>:</span><span class=n>integer</span> <span class=k>for</span> <span class=n>integer</span><span class=p>,</span><span class=n>token</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>all_words</span><span class=p>)}</span>
</span></span></code></pre></div><p>apply this vocabulary to convert new text into token IDs and turn token IDs into textã€‚</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%204.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># implement a complete tokenizer class</span>
</span></span><span class=line><span class=cl><span class=c1># with an encode method that splits text into tokens </span>
</span></span><span class=line><span class=cl><span class=c1># and carries out the string-to-integer mapping to produce tokenIDs via the vocabulary</span>
</span></span><span class=line><span class=cl><span class=c1># a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleTokenizerV1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>str_to_int</span> <span class=o>=</span> <span class=n>vocab</span> 
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>int_to_str</span> <span class=o>=</span> <span class=p>{</span><span class=n>i</span><span class=p>:</span><span class=n>s</span> <span class=k>for</span> <span class=n>s</span><span class=p>,</span><span class=n>i</span> <span class=ow>in</span> <span class=n>vocab</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span> 
</span></span><span class=line><span class=cl>		<span class=c1># .items() return all key-value pairs of dict.such as dict_items([(&#39;hello&#39;, 1), (&#39;world&#39;, 2)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>		<span class=n>preprocessed</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;([,.?_!&#34;()</span><span class=se>\&#39;</span><span class=s1>]|--|\s)&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>preprocessed</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>		<span class=n>item</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span> <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>preprocessed</span> <span class=k>if</span> <span class=n>item</span><span class=o>.</span><span class=n>strip</span><span class=p>()]</span>
</span></span><span class=line><span class=cl>		<span class=c1># ifåçš„stripæ˜¯åˆ¤æ–­æœ‰æ²¡æœ‰ç©ºå†…å®¹ï¼Œæ¯”å¦‚å…¨ç©ºæ ¼æˆ–è€…æ¢è¡Œç¬¦ï¼Œåˆ¤æ–­falseè¿‡æ»¤æ‰</span>
</span></span><span class=line><span class=cl>		<span class=c1># æœ€å‰é¢çš„stripæ˜¯å¦‚æœä¸æ˜¯å…¨ç©ºå†…å®¹ï¼Œå°±å¯¹å†…å®¹è¿›è¡Œæ¸…æ´—å»æ‰ç©ºæ ¼æ¢è¡Œç¬¦ä¿ç•™æ¸¸æ³³å†…å®¹</span>
</span></span><span class=line><span class=cl>		<span class=c1># æ²¡æœ‰ifåçš„stripå‰é¢çš„stripèƒ½ä¸èƒ½æ¸…æ´—æ‰å…¨ç©ºçš„å†…å®¹,ä¸è¡Œã€‚â€œ    â€.strip()ä¼šå˜æˆâ€œâ€</span>
</span></span><span class=line><span class=cl>		<span class=n>ids</span> <span class=o>=</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>str_to_int</span><span class=p>[</span><span class=n>s</span><span class=p>]</span> <span class=k>for</span> <span class=n>s</span> <span class=ow>in</span> <span class=n>preprocessed</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>ids</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>ids</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>		<span class=n>text</span> <span class=o>=</span> <span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>int_to_str</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>ids</span><span class=p>])</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>		<span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\s+([,.?!&#34;()</span><span class=se>\&#39;</span><span class=s1>])&#39;</span><span class=p>,</span> <span class=sa>r</span><span class=s1>&#39;\1&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=c1># åœ¨å­—ç¬¦ä¸² string ä¸­ï¼ŒæŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆ pattern çš„å­ä¸²ï¼Œå¹¶ç”¨ repl æ›¿æ¢æ‰ã€‚</span>
</span></span><span class=line><span class=cl>		<span class=c1># åœ¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ï¼ˆre.subï¼‰é‡Œï¼Œ\1 è¡¨ç¤ºï¼šå¼•ç”¨ç¬¬ 1 ä¸ªæ‹¬å·é‡Œæ•è·çš„å†…å®¹ã€‚</span>
</span></span><span class=line><span class=cl>		<span class=c1># \s+ â†’ ä¸€ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ï¼‰</span>
</span></span><span class=line><span class=cl>		<span class=c1># ([,.?!&#34;()&#39;]) â†’ æ•è·æ‹¬å·å†…çš„ä»»æ„ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·</span>
</span></span><span class=line><span class=cl>		<span class=c1># \1 â†’ æ­£åˆ™ä¸­ç¬¬ä¸€ä¸ªæ‹¬å·æ•è·çš„å†…å®¹ï¼ˆå³æ ‡ç‚¹ç¬¦å·æœ¬èº«ï¼‰</span>
</span></span><span class=line><span class=cl>		<span class=c1># ä½œç”¨å°±æ˜¯ï¼šå»æ‰æ ‡ç‚¹å‰é¢çš„å¤šä½™ç©ºæ ¼ã€‚</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>text</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>In [18]: a = [&#39;Hello&#39;, &#39;world&#39;, &#39; ! &#39;, &#39;     &#39;]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>In [19]: [item.strip() for item in a]
</span></span></span><span class=line><span class=cl><span class=s2>Out[19]: [&#39;Hello&#39;, &#39;world&#39;, &#39;!&#39;, &#39;&#39;]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>In [20]: [item.strip() for item in a if item.strip()]
</span></span></span><span class=line><span class=cl><span class=s2>Out[20]: [&#39;Hello&#39;, &#39;world&#39;, &#39;!&#39;]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>In [21]: [item for item in a if item.strip()]
</span></span></span><span class=line><span class=cl><span class=s2>Out[21]: [&#39;Hello&#39;, &#39;world&#39;, &#39; ! &#39;]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>Using the SimpleTokenizerV1 Python class, we can now instantiate new tokenizer objects via an existing vocabulary, which we can then use to encode and decode text</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%205.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>SimpleTokenizerV1</span><span class=p>(</span><span class=n>vocab</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;&#34;It&#39;s the last he painted, you know,&#34; 
</span></span></span><span class=line><span class=cl><span class=s2> Mrs. Gisburn said with pardonable pride.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>ids</span><span class=p>))</span>
</span></span></code></pre></div><p><em>2.4 Adding special context tokens</em></p><p>modify the tokenizer to handle unknown words and address the usage and addition of special context tokensã€‚</p><p>special tokens including markers for unknown words and document boundaries, &lt;|unk|> and &lt;|endoftext|></p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%206.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># add &lt;unk&gt; and &lt;|endoftext|&gt; to list of all unique words.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>all_tokens</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>set</span><span class=p>(</span><span class=n>preprocessed</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>all_tokens</span><span class=o>.</span><span class=n>extend</span><span class=p>([</span><span class=s2>&#34;&lt;|endoftext|&gt;&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;|unk|&gt;&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>vocab</span> <span class=o>=</span> <span class=p>{</span><span class=n>token</span><span class=p>:</span><span class=n>integer</span> <span class=k>for</span> <span class=n>integer</span><span class=p>,</span><span class=n>token</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>all_tokens</span><span class=p>)}</span>
</span></span></code></pre></div><p>A simple text tokenizer that handles unknown words</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleTokenizerV2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>str_to_int</span> <span class=o>=</span> <span class=n>vocab</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>int_to_str</span> <span class=o>=</span> <span class=p>{</span> <span class=n>i</span><span class=p>:</span><span class=n>s</span> <span class=k>for</span> <span class=n>s</span><span class=p>,</span><span class=n>i</span> <span class=ow>in</span> <span class=n>vocab</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>preprocessed</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;([,.:;?_!&#34;()</span><span class=se>\&#39;</span><span class=s1>]|--|\s)&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>preprocessed</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>		<span class=n>item</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span> <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>preprocessed</span> <span class=k>if</span> <span class=n>item</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>preprocessed</span> <span class=o>=</span> <span class=p>[</span><span class=n>item</span> <span class=k>if</span> <span class=n>item</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>str_to_int</span> 
</span></span><span class=line><span class=cl>		<span class=k>else</span> <span class=s2>&#34;&lt;|unk|&gt;&#34;</span> <span class=k>for</span> <span class=n>item</span> <span class=ow>in</span> <span class=n>preprocessed</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>ids</span> <span class=o>=</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>str_to_int</span><span class=p>[</span><span class=n>s</span><span class=p>]</span> <span class=k>for</span> <span class=n>s</span> <span class=ow>in</span> <span class=n>preprocessed</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>ids</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>ids</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>text</span> <span class=o>=</span> <span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>int_to_str</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>ids</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=n>text</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;\s+([,.:;?!&#34;()</span><span class=se>\&#39;</span><span class=s1>])&#39;</span><span class=p>,</span> <span class=sa>r</span><span class=s1>&#39;\1&#39;</span><span class=p>,</span> <span class=n>text</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>text</span>
</span></span><span class=line><span class=cl>		
</span></span></code></pre></div><p><em>2.5 Byte pair encoding</em></p><p>Python open source library called <em>tiktoken</em> (<a href=https://github.com/openai/tiktoken)>https://github.com/openai/tiktoken)</a>, which implements the BPE algorithm very efficiently based on source code in Rust.</p><p><code>pip install tiktoken</code></p><p>The code we will use is based on tiktoken 0.7.0.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>importlib.metadata</span> <span class=kn>import</span> <span class=n>version</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;tiktoken version:&#34;</span><span class=p>,</span> <span class=n>version</span><span class=p>(</span><span class=s2>&#34;tiktoken&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># instantiate the BPE tokenizer from tiktoken</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># an encode method:</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces&#34;</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;of someunknownPlace.&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>integers</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>allowed_special</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;&lt;|endoftext|&gt;&#34;</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>integers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># the decode method</span>
</span></span><span class=line><span class=cl><span class=n>strings</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>integers</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>strings</span><span class=p>)</span>
</span></span></code></pre></div><p>the BPE tokenizer has a total vocabulary size of 50,257</p><p>BPE breaks down words that arenâ€™t in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words.</p><p><em>2.6 Data sampling with a sliding window</em></p><p><em>2.7 Creating token embeddings</em></p><p>convert the token IDs into embedding vectors</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%207.png></p><p>how the token ID to embedding vector conversion</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># the embedding layer is essentially a lookup operation </span>
</span></span><span class=line><span class=cl><span class=c1># that retrieves rows from the embedding layerâ€™s weight matrix via a token ID.</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>embedding_layer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embedding_layer</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># apply embedding layer to a token ID to obtain the embedding vector</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embedding_layer</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>3</span><span class=p>])))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embedding_layer</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>1</span><span class=p>])))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix</span>
</span></span></code></pre></div><p><em>2.8 Encoding word positions</em></p><p>it is helpful to inject additional position information into the LLM</p><p>two broad categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings</p><p>OpenAIâ€™s GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original transformer model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># token embedding</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=mi>50257</span>
</span></span><span class=line><span class=cl><span class=n>output_dim</span> <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl><span class=n>token_embedding_layer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># position embedding</span>
</span></span><span class=line><span class=cl><span class=n>context_length</span> <span class=o>=</span> <span class=n>max_length</span>
</span></span><span class=line><span class=cl><span class=n>pos_embedding_layer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>context_length</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pos_embeddings</span> <span class=o>=</span> <span class=n>pos_embedding_layer</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>context_length</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># add pos_embeddings to token_embeddings</span>
</span></span><span class=line><span class=cl><span class=n>input_embeddings</span> <span class=o>=</span> <span class=n>token_embeddings</span> <span class=o>+</span> <span class=n>pos_embeddings</span>
</span></span></code></pre></div><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%208.png></p><h3 id=coding-attention-mechanisms><em>Coding attention mechanisms</em><a hidden class=anchor aria-hidden=true href=#coding-attention-mechanisms>#</a></h3><p>ğŸsimplified attention mechanism</p><p>ğŸadd a causal attention mask to prevent the LLM from accessing future tokens</p><p>ğŸadd a dropout mask to reduce overfitting in LLMs</p><p>ğŸmulti-head attention: multiple instances of causal attention</p><p>ğŸcreating multi-head attention modules involves batched matrix multiplications</p><p><em>3.4.2 Implementing a compact self-attention Python class</em></p><ol><li>initializes trainable weight matrices (W_query, W_key, and W_value)</li><li>compute the attention scores (attn_scores) by multiplying queries and keys</li><li>normalizing these scores using softmax to get attn_weights</li><li>create a context vector by weighting the values with these attn_weights</li></ol><p>$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V$</p><p>a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(&mldr;)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SelfAttention_v2</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span> <span class=c1># shape of x: (seq_len, d_in)</span>
</span></span><span class=line><span class=cl>		<span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># shape of keys: (seq_len, d_out)</span>
</span></span><span class=line><span class=cl>		<span class=n>queries</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># shape of keys: (seq_len, d_out)</span>
</span></span><span class=line><span class=cl>		<span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># shape of keys: (seq_len, d_out)</span>
</span></span><span class=line><span class=cl>		<span class=n>attn_scores</span> <span class=o>=</span> <span class=n>queries</span> <span class=o>@</span> <span class=n>keys</span><span class=o>.</span><span class=n>T</span> <span class=c1># (seq_len, seq_len)</span>
</span></span><span class=line><span class=cl>		<span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>attn_scores</span> <span class=o>/</span> <span class=n>keys</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span> <span class=c1># keys.shape[-1]: d_out</span>
</span></span><span class=line><span class=cl>		<span class=c1># torch.softmax(......, dim=-1),æŒ‰æœ€åä¸€ç»´ç®—softmaxï¼Œä¹Ÿå°±æ˜¯æŒ‰è¡Œ</span>
</span></span><span class=line><span class=cl>		<span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>values</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>context_vec</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl><span class=c1># inputs contains six embedding vectors</span>
</span></span><span class=line><span class=cl><span class=c1># results contains six context vectors	</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sa_v1</span> <span class=o>=</span> <span class=n>SelfAttention_v1</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sa_v1</span><span class=p>(</span><span class=n>inputs</span><span class=p>))</span>
</span></span></code></pre></div><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%209.png></p><p><em>3.5 Hiding future words with causal attention</em></p><p>Causal attention, also known as masked attention è®©æ¨¡å‹åªå…³æ³¨sequenceä¸­çš„previous å’Œ current input</p><p>mask out the future tokens</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2010.png></p><p>dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors.</p><p>apply the dropout mask after computing the attention weightsæ›´å¸¸è§ã€‚</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2011.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>CausalAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>dropout</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>d_out</span> <span class=o>=</span> <span class=n>d_out</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=s1>&#39;mask&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>torch</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>),</span>
</span></span><span class=line><span class=cl>		<span class=n>diagonal</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span> 
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>b</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span> 
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>queries</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>attn_scores</span> <span class=o>=</span> <span class=n>queries</span> <span class=o>@</span> <span class=n>keys</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=c1># creating a mask with 1s above the diagonal </span>
</span></span><span class=line><span class=cl>		<span class=c1># and then replacing these 1s with negative infinity (-inf) values</span>
</span></span><span class=line><span class=cl>		<span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill_</span><span class=p>(</span> 
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>mask</span><span class=o>.</span><span class=n>bool</span><span class=p>()[:</span><span class=n>num_tokens</span><span class=p>,</span> <span class=p>:</span><span class=n>num_tokens</span><span class=p>],</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>inf</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>attn_scores</span> <span class=o>/</span> <span class=n>keys</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>values</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>context_vec</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>context_length</span> <span class=o>=</span> <span class=n>batch</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>ca</span> <span class=o>=</span> <span class=n>CausalAttention</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>context_vecs</span> <span class=o>=</span> <span class=n>ca</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;context_vecs.shape:&#34;</span><span class=p>,</span> <span class=n>context_vecs</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><p><em>3.6 Extending single-head attention to multi-head attention</em></p><p>processing the heads in sequential</p><p>multiple heads are implemented by creating a list of CausalAttention objects (self.heads)</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2012.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttentionWrapper</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>dropout</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=p>[</span><span class=n>CausalAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>qkv_bias</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_heads</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>head</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><p>processing the heads in parallel</p><p>splits the input into multiple heads by reshaping the projected query, key, and value
tensors and then combines the results from these heads after computing attention</p><p>split the d_out dimension into num_heads and head_dim, where head_dim = d_out / num_heads.</p><p>This splitting is then achieved using the .view method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens, num_heads, head_dim)</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2013.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>		<span class=n>context_length</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=k>assert</span> <span class=p>(</span><span class=n>d_out</span> <span class=o>%</span> <span class=n>num_heads</span> <span class=o>==</span> <span class=mi>0</span><span class=p>),</span> \
</span></span><span class=line><span class=cl>		<span class=s2>&#34;d_out must be divisible by num_heads&#34;</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>d_out</span> <span class=o>=</span> <span class=n>d_out</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>d_out</span> <span class=o>//</span> <span class=n>num_heads</span> 
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>W_value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_out</span><span class=p>,</span> <span class=n>d_out</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=s2>&#34;mask&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>torch</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>),</span>
</span></span><span class=line><span class=cl>		<span class=n>diagonal</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>b</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>		<span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>queries</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>keys</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>b</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>values</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>b</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>queries</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>view</span><span class=p>(</span> 
</span></span><span class=line><span class=cl>		<span class=n>b</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> 
</span></span><span class=line><span class=cl>		<span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>keys</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>queries</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>values</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>attn_scores</span> <span class=o>=</span> <span class=n>queries</span> <span class=o>@</span> <span class=n>keys</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>mask_bool</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mask</span><span class=o>.</span><span class=n>bool</span><span class=p>()[:</span><span class=n>num_tokens</span><span class=p>,</span> <span class=p>:</span><span class=n>num_tokens</span><span class=p>]</span> 
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill_</span><span class=p>(</span><span class=n>mask_bool</span><span class=p>,</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>inf</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>attn_scores</span> <span class=o>/</span> <span class=n>keys</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>context_vec</span> <span class=o>=</span> <span class=p>(</span><span class=n>attn_weights</span> <span class=o>@</span> <span class=n>values</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>context_vec</span> <span class=o>=</span> <span class=n>context_vec</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>b</span><span class=p>,</span> <span class=n>num_tokens</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_out</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>context_vec</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span><span class=p>(</span><span class=n>context_vec</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>context_vec</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>batch</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl><span class=n>d_out</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>mha</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>context_vecs</span> <span class=o>=</span> <span class=n>mha</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>context_vecs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;context_vecs.shape:&#34;</span><span class=p>,</span> <span class=n>context_vecs</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=implementing-a-gpt-model-from-scratch-to-generate-text><em>Implementing a GPT model from scratch to generate text</em><a hidden class=anchor aria-hidden=true href=#implementing-a-gpt-model-from-scratch-to-generate-text>#</a></h3><p><em>This chapter covers</em></p><ul><li>Coding a GPT-like large language model (LLM) that can be trained to generate human-like text</li><li>Normalizing layer activations to stabilize neural network training</li><li>Adding shortcut connections in deep neural networks</li><li>Implementing transformer blocks to create GPT models of various sizes</li><li>Computing the number of parameters and storage requirements of GPT models</li></ul><p><em>4.1 Coding an LLM architecture</em></p><p>parametersæŒ‡çš„æ˜¯trainable weights of the model</p><p>a GPT placeholder architecture (DummyGPTModel)</p><p>the order in which we tackle the individual concepts required to code the final GPT architecture</p><p>å…ˆcodeå‡ºä¸€ä¸ªGPT placeholder architecture calling DummyGPTModelï¼Œç„¶åå¾—åˆ°the individual core pieces ï¼Œæœ€ç»ˆassemblingèµ·æ¥ã€‚</p><p>ä¸€ä¸ªDummyGPTModelåŒ…æ‹¬token embeddings , positional embedding , dropout , ä¸€ç³»åˆ—çš„transformer blocks(DummyTransformerBlock) æœ€åä¸€ä¸ªLayer Normalization(DummyLayerNorm)å’Œä¸€ä¸ªLinear output layer</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2014.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>GPT_CONFIG_124M</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;vocab_size&#34;</span><span class=p>:</span> <span class=mi>50257</span><span class=p>,</span> <span class=c1># Vocabulary size</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;context_length&#34;</span><span class=p>:</span> <span class=mi>1024</span><span class=p>,</span> <span class=c1># modelèƒ½å¤Ÿå¤„ç†çš„æœ€å¤§tokenå’Œpositional embeddingæ•°é‡</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;emb_dim&#34;</span><span class=p>:</span> <span class=mi>768</span><span class=p>,</span> <span class=c1># æ¯ä¸ªTokençš„Embedding size</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;n_heads&#34;</span><span class=p>:</span> <span class=mi>12</span><span class=p>,</span> <span class=c1># Number of attention heads</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;n_layers&#34;</span><span class=p>:</span> <span class=mi>12</span><span class=p>,</span> <span class=c1># Transformer Blockçš„æ•°é‡</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;drop_rate&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>,</span> <span class=c1># Dropout rate</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;qkv_bias&#34;</span><span class=p>:</span> <span class=kc>False</span> <span class=c1># Query-Key-Value bias</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=c1># A placeholder GPT model architecture class</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>	<span class=k>class</span> <span class=nc>DummyGPTModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>tok_emb</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;vocab_size&#34;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>pos_emb</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;context_length&#34;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>drop_emb</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;drop_rate&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>trf_blocks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span> 
</span></span><span class=line><span class=cl>		<span class=o>*</span><span class=p>[</span><span class=n>DummyTransformerBlock</span><span class=p>(</span><span class=n>cfg</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;n_layers&#34;</span><span class=p>])]</span> 
</span></span><span class=line><span class=cl>		<span class=p>)</span> 
</span></span><span class=line><span class=cl>		<span class=c1># *èµ·unpackçš„ä½œç”¨ï¼Œå…·ä½“æ˜¯å°†åˆ—è¡¨é‡Œçš„å…ƒç´ ç»™ä¸€ä¸€æ‹¿å‡ºæ¥ã€‚</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>final_norm</span> <span class=o>=</span> <span class=n>DummyLayerNorm</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>])</span> 
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>out_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;vocab_size&#34;</span><span class=p>],</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span> <span class=o>=</span> <span class=n>in_idx</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>		<span class=n>tok_embeds</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tok_emb</span><span class=p>(</span><span class=n>in_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>pos_embeds</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_emb</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>in_idx</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>tok_embeds</span> <span class=o>+</span> <span class=n>pos_embeds</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>drop_emb</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>trf_blocks</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>final_norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>logits</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DummyTransformerBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DummyLayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>normalized_shape</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-5</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p><em>4.2 Normalizing activations with layer normalization</em></p><p>ä¸ºä»€ä¹ˆç”¨batch normalizationè€Œä¸æ˜¯layer normalizationï¼Ÿ</p><p>è®­ç»ƒæœ‰è®¸å¤šlayersçš„deep neural networkæ—¶ä¼šé‡åˆ°vanishing or exploding gradientsçš„é—®é¢˜ã€‚</p><p>å®ç°Layer normalizationå¯ä»¥æå‡è®­ç»ƒçš„stabilityå’Œefficiencyï¼Œ</p><p>layer normalizationä¸€èˆ¬æ˜¯æ”¾åœ¨multi-head attention moduleçš„å‰å</p><p>dim=-1è¡¨ç¤ºå‘é‡çš„æœ€åä¸€ç»´ï¼Œå¯¹äºä¸€ä¸ªtwo dimensional tensoræ¥è¯´ä¹Ÿå°±æ˜¯å‘é‡çš„columnsã€‚å¯¹äºä¸€ä¸ªä¸‰ç»´å‘é‡[bs, seq_len, embedding_size]æ¥è¯´æœ€åä¸€ç»´å°±æ˜¯æ¯ä¸ªtokençš„embeddin</p><p>_size.</p><p>layer normalizationçš„æ“ä½œï¼šout_norm = (out - mean) / torch.sqrt(var).</p><p>layer normalizationä¸€èˆ¬æ˜¯åœ¨è¾“å…¥tensorçš„last dimensionæ“ä½œçš„ï¼Œè¿™ä»£è¡¨embedding dimension (emb_dim)ã€‚</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>	<span class=k>class</span> <span class=nc>LayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=mf>1e-5</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>shift</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># é‡‡ç”¨Biased varianceæœ‰åä¼°è®¡æ¥æ±‚æ ·æœ¬æ–¹å·®ï¼Œå› ä¸ºç»´åº¦768è¶³å¤Ÿå¤§ï¼ŒåŒºåˆ«ä¸å¤§</span>
</span></span><span class=line><span class=cl>		<span class=n>var</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>unbiased</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>norm_x</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># epsæ˜¯æå°å€¼ï¼Œé˜²æ­¢division by zeroçš„æƒ…å†µ</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>*</span> <span class=n>norm_x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>shift</span>
</span></span><span class=line><span class=cl>		<span class=c1># scale å’Œ shiftæ˜¯ä¸¤ä¸ªtrainable parameters,åˆå§‹åˆ†åˆ«æ˜¯1ï¼Œ0ï¼Œ</span>
</span></span><span class=line><span class=cl>		<span class=c1># è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚æœè¢«åˆ¤å®šè°ƒæ•´ä»–èƒ½æå‡performanceï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨çš„è°ƒæ•´ä»–ã€‚</span>
</span></span></code></pre></div><p>æ–¹å·®ä¸æ ·æœ¬æ–¹å·®ï¼Œä»¥åŠæ ·æœ¬æ–¹å·®çš„æœ‰åä¼°è®¡ä¸æ— åä¼°è®¡ã€‚</p><p>æ–¹å·®æ˜¯é™¤nï¼Œä¸ºä»€ä¹ˆæ ·æœ¬æ–¹å·®é™¤nåè€Œæ˜¯æœ‰åä¼°è®¡ï¼Œæ ·æœ¬æ–¹å·®æ— åä¼°è®¡æ˜¯é™¤ä»¥n-1ï¼Ÿå› ä¸ºé‡‡æ ·è¿‡ç¨‹ä¸­ä¼šä½ä¼°æ–¹å·®ï¼Œæ‰€ä»¥è¦é€šè¿‡è´å¡å°”ä¿®æ­£æ¥ä¿®æ­£æ–¹å·®ã€‚æ–¹å·®æ˜¯ååº”æ•°æ®ç¦»æ•£ç¨‹åº¦çš„ï¼Œä»æ‰€æœ‰çš„æ•°æ®ä¸­é‡‡æ ·ä¸€äº›æ ·æœ¬ï¼Œç”¨æ ·æœ¬çš„æ ·æœ¬æ–¹å·®æ¥ååº”æ€»ä½“æ–¹å·®çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯é‡‡æ ·æ ·æœ¬çš„è¿‡ç¨‹è‚¯å®šæ˜¯æ¦‚ç‡è¶Šå¤§çš„è¶Šå®¹æ˜“é‡‡æ ·ï¼Œå°±å¯¼è‡´é‡‡æ ·çš„æ ·æœ¬ç¦»æ•£ç¨‹åº¦æ›´é›†ä¸­ï¼Œå½“ç„¶å¦‚æœé‡‡æ ·çš„æ•°ç›®nè¶³å¤Ÿå¤§ï¼Œé‡‡æ ·çš„æ ·æœ¬çš„åˆ†å¸ƒå°±æ— é™åˆ†å¸ƒæ€»ä½“çš„åˆ†å¸ƒäº†ã€‚ä»¥å¦‚ä¸‹æ­£å¤ªåˆ†å¸ƒä¸ºä¾‹ã€‚</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2015.png></p><p><em>4.3 Implementing a feed forward network with GELU activations</em></p><p>ä¸ºä»€ä¹ˆç”¨GELUï¼ˆé¸¡è·¯ï¼‰æ¿€æ´»å‡½æ•°è€Œä¸æ˜¯reluæ¿€æ´»å‡½æ•°ï¼Ÿå› ä¸ºreluæ¿€æ´»å‡½æ•°åœ¨0çš„åœ°æ–¹ä¸å¯å¾®ã€‚</p><p>dead neuronsï¼šå½“è¾“å…¥å°äº0æ—¶è¾“å‡ºæ°¸è¿œæ˜¯0å¯¹å­¦ä¹ æ²¡æœ‰ä»€ä¹ˆè´¡çŒ®ï¼Œæ‰€ä»¥å«dead neuronsã€‚</p><p>GELU(x) = xâ‹…Î¦(x), where Î¦(x) is the cumulative distribution function of the standard Gaussian distributionã€‚</p><p>standard Gaussian distribution</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2016.png></p><p>standard Gaussian distributionçš„CDF</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2017.png></p><p>(the original GPT-2 model was also trained with this approximation, which was found via curve fittingï¼‰</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2018.png></p><p>m = nn.GELU()</p><p>An implementation of the GELU activation function</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GELU</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>pi</span><span class=p>))</span> <span class=o>*</span> 
</span></span><span class=line><span class=cl>		<span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=mf>0.044715</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=p>))</span>
</span></span></code></pre></div><p>A feed forward neural network module</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>],</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>		<span class=n>GELU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>		<span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4</span> <span class=o>*</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2019.png></p><p><em>4.4 Adding shortcut connections</em></p><p>short connectionså’Œresidual connectionæ˜¯ä¸€å›äº‹ï¼Œç”¨æ¥è§£å†³vanishing gredient problemã€‚</p><p><em>vanishing gradient problem</em> æ˜¯æŒ‡åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦è¿‡å°å¯¼è‡´å­¦ä¹ åœæ»ä¸å‰ï¼Œconvergence delayã€‚</p><p>ä¸åŠ Skip Connection åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦ä¼šè¶Šæ¥è¶Šå°ã€‚skip connectionå¯ä»¥ä¸ºæ¢¯åº¦åˆ›å»ºä¸€ä¸ªå¦å¤–çš„æ›´çŸ­çš„è·¯å¾„ï¼Œé€šè¿‡skipping one or more layersè®©gradient flow</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2020.png></p><p><img alt="Visualizing the Loss Landscape of Neural Nets" loading=lazy src=/attachment/Transformer/image%2021.png></p><p>Visualizing the Loss Landscape of Neural Nets</p><p>ä¸åŠ Skip Connectionæœ‰å¾ˆå¤šçš„å±€éƒ¨æœ€å°å€¼ï¼Œä¼šå¯¼è‡´ä¼˜åŒ–å›°éš¾</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ExampleDeepNeuralNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>layer_sizes</span><span class=p>,</span> <span class=n>use_shortcut</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>use_shortcut</span> <span class=o>=</span> <span class=n>use_shortcut</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>		<span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>layer_sizes</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>layer_sizes</span><span class=p>[</span><span class=mi>1</span><span class=p>]),</span> 
</span></span><span class=line><span class=cl>		<span class=n>GELU</span><span class=p>()),</span>
</span></span><span class=line><span class=cl>		<span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>layer_sizes</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>layer_sizes</span><span class=p>[</span><span class=mi>2</span><span class=p>]),</span> 
</span></span><span class=line><span class=cl>		<span class=n>GELU</span><span class=p>()),</span>
</span></span><span class=line><span class=cl>		<span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>layer_sizes</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=n>layer_sizes</span><span class=p>[</span><span class=mi>3</span><span class=p>]),</span> 
</span></span><span class=line><span class=cl>		<span class=n>GELU</span><span class=p>()),</span>
</span></span><span class=line><span class=cl>		<span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>layer_sizes</span><span class=p>[</span><span class=mi>3</span><span class=p>],</span> <span class=n>layer_sizes</span><span class=p>[</span><span class=mi>4</span><span class=p>]),</span> 
</span></span><span class=line><span class=cl>		<span class=n>GELU</span><span class=p>()),</span>
</span></span><span class=line><span class=cl>		<span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>layer_sizes</span><span class=p>[</span><span class=mi>4</span><span class=p>],</span> <span class=n>layer_sizes</span><span class=p>[</span><span class=mi>5</span><span class=p>]),</span> 
</span></span><span class=line><span class=cl>		<span class=n>GELU</span><span class=p>())</span>
</span></span><span class=line><span class=cl>		<span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>layer_output</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>			<span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_shortcut</span> <span class=ow>and</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span> <span class=o>==</span> <span class=n>layer_output</span><span class=o>.</span><span class=n>shape</span><span class=p>:</span> 
</span></span><span class=line><span class=cl>				<span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>layer_output</span>
</span></span><span class=line><span class=cl>			<span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>				<span class=n>x</span> <span class=o>=</span> <span class=n>layer_output</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>a function that computes the gradients</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>print_gradients</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>	<span class=n>target</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>0.</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>	<span class=n>loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=n>loss</span> <span class=o>=</span> <span class=n>loss</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>	<span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=s1>&#39;weight&#39;</span> <span class=ow>in</span> <span class=n>name</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2> has gradient mean of </span><span class=si>{</span><span class=n>param</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span> 
</span></span></code></pre></div><p><em>4.5 Connecting attention and linear layers in a transformer block</em></p><p>implement the <em>transformer block åŒ…æ‹¬</em> multi-head attention, layer normalization, dropout, feed forward layers, and GELU activationsã€‚</p><p>TransformerBlockçš„æ ¸å¿ƒæ˜¯åŒ…æ‹¬ a multi-head attention mechanism (MultiHeadAttention) and a feed forward network (FeedForward)ã€‚å…¶ä¸­Layer normalization (LayerNorm)æ˜¯åœ¨ä»¥ä¸Šä¸¤éƒ¨åˆ†çš„å‰é¢ï¼Œdropoutæ˜¯åœ¨ä»¥ä¸Šä¸¤éƒ¨åˆ†çš„åé¢ã€‚layer normalizationåœ¨å‰é¢æ˜¯å«Pre-LayerNormï¼Œåœ¨åŸè®ºæ–‡ä¸­LayerNormæ˜¯åœ¨åé¢å«Post-LayerNorm</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2022.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>att</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>			<span class=n>d_in</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>			<span class=n>d_out</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>			<span class=n>context_length</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;context_length&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>			<span class=n>num_heads</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;n_heads&#34;</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>			<span class=n>dropout</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;drop_rate&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>			<span class=n>qkv_bias</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;qkv_bias&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>ff</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>(</span><span class=n>cfg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>drop_shortcut</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;drop_rate&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>		<span class=n>shortcut</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>att</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>drop_shortcut</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>shortcut</span> 
</span></span><span class=line><span class=cl>		<span class=n>shortcut</span> <span class=o>=</span> <span class=n>x</span> 
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ff</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>drop_shortcut</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>shortcut</span> 
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p><em>4.6 Coding the GPT model</em></p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2023.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GPTModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>tok_emb</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;vocab_size&#34;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>pos_emb</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;context_length&#34;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>drop_emb</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;drop_rate&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>trf_blocks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=o>*</span><span class=p>[</span><span class=n>TransformerBlock</span><span class=p>(</span><span class=n>cfg</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;n_layers&#34;</span><span class=p>])])</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>final_norm</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>out_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;emb_dim&#34;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s2>&#34;vocab_size&#34;</span><span class=p>],</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span> <span class=o>=</span> <span class=n>in_idx</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>		<span class=n>tok_embeds</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tok_emb</span><span class=p>(</span><span class=n>in_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		
</span></span><span class=line><span class=cl>		<span class=n>pos_embeds</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_emb</span><span class=p>(</span>
</span></span><span class=line><span class=cl>		<span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>seq_len</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>in_idx</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>tok_embeds</span> <span class=o>+</span> <span class=n>pos_embeds</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>drop_emb</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>trf_blocks</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>final_norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>logits</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># è®¡ç®—modelçš„parametersæ•°é‡</span>
</span></span><span class=line><span class=cl><span class=n>total_params</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Total number of parameters: </span><span class=si>{</span><span class=n>total_params</span><span class=si>:</span><span class=s2>,</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># è®¡ç®—modelçš„parameterså çš„å†…å­˜å¤§å°</span>
</span></span><span class=line><span class=cl><span class=c1># Calculates the total size in bytes (assuming float32, 4 bytes per parameter)</span>
</span></span><span class=line><span class=cl><span class=n>total_size_bytes</span> <span class=o>=</span> <span class=n>total_params</span> <span class=o>*</span> <span class=mi>4</span> 
</span></span><span class=line><span class=cl><span class=c1># Converts to megabytes</span>
</span></span><span class=line><span class=cl><span class=n>total_size_mb</span> <span class=o>=</span> <span class=n>total_size_bytes</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Total size of the model: </span><span class=si>{</span><span class=n>total_size_mb</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> MB&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>ä»¥ä¸Šæ–¹å¼è®¡ç®—çš„æ€»å‚æ•°é‡æ˜¯163,009,536ï¼Œå®é™…GPT-2çš„å‚æ•°é‡æ˜¯124,412,160ã€‚åŸå› æ˜¯GPT-2é‡Œæœ‰ä¸ªå‚æ•°ç»‘å®šï¼ˆweight tyingï¼‰çš„æŠ€æœ¯ã€‚å…·ä½“åšæ³•å°±æ˜¯åœ¨output layeré‡Œå¤ç”¨äº†the token embedding layerçš„æƒé‡ã€‚ä¸ºä»€ä¹ˆè¦è¿™æ ·åšäº†ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªå±‚çš„ç»´åº¦æ˜¯vocabulary size50, 257éå¸¸çš„å·¨å¤§ã€‚</p><p><em>4.7 Generating text</em></p><p><em>greedy decoding: å–æ¦‚ç‡æœ€å¤§çš„ä½ç½®å¤„çš„tokenã€‚</em></p><p>ç”¨softmax functionå»å°†logitsè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œç”¨torch.argmaxé€‰å‡ºæ¦‚ç‡æœ€å¤§å¤„çš„ç´¢å¼•ã€‚</p><p><img alt=image.png loading=lazy src=/attachment/Transformer/image%2024.png></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_text_simple</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=p>,</span> <span class=n>context_size</span><span class=p>):</span> 
</span></span><span class=line><span class=cl>	<span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>		max_new_tokens:å¸Œæœ›ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡ã€‚
</span></span></span><span class=line><span class=cl><span class=s1>		context_size:æ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
</span></span></span><span class=line><span class=cl><span class=s1>	&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>	<span class=c1># å¾ªç¯max_new_tokensæ¬¡ï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ–°çš„token</span>
</span></span><span class=line><span class=cl>	 <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_new_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		 <span class=c1># å—é™äºæ¨¡å‹èƒ½å¤„ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦context_size,é€‰æœ€æ–°çš„context_sizeä¸ªtokenã€‚</span>
</span></span><span class=line><span class=cl>		 <span class=n>idx_cond</span> <span class=o>=</span> <span class=n>idx</span><span class=p>[:,</span> <span class=o>-</span><span class=n>context_size</span><span class=p>:]</span> 
</span></span><span class=line><span class=cl>		 <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>			 <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>idx_cond</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		 <span class=c1># é€‰å‡ºæ¨¡å‹generateçš„token: (bs, n_tokens, vocab_size) ---&gt; (bs, vocab_size)</span>
</span></span><span class=line><span class=cl>		 <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span> 
</span></span><span class=line><span class=cl>		 <span class=n>probas</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span> <span class=c1># logits ---&gt; Probability distribution</span>
</span></span><span class=line><span class=cl>		 <span class=n>idx_next</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>probas</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># (bs,1)</span>
</span></span><span class=line><span class=cl>		 <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>idx</span><span class=p>,</span> <span class=n>idx_next</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> 
</span></span></code></pre></div><h3 id=pretraining-on-unlabeled-data><em>Pretraining on unlabeled data</em><a hidden class=anchor aria-hidden=true href=#pretraining-on-unlabeled-data>#</a></h3><p><em>5.1.1 Using GPT to generate text</em></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>chapter04</span> <span class=kn>import</span> <span class=n>generate_text_simple</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>text_to_token_ids</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>encoded</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>allowed_special</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;&lt;|endoftext|&gt;&#39;</span><span class=p>})</span>
</span></span><span class=line><span class=cl>	<span class=n>encoded_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>encoded</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>	<span class=c1># .unsqueeze(0) adds the batch dimension</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>encoded_tensor</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>token_ids_to_text</span><span class=p>(</span><span class=n>token_ids</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>flat</span> <span class=o>=</span> <span class=n>token_ids</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>	<span class=c1># Removes batch dimension </span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>flat</span><span class=o>.</span><span class=n>tolist</span><span class=p>())</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl><span class=n>start_context</span> <span class=o>=</span> <span class=s2>&#34;Every effort moves you&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>token_ids</span> <span class=o>=</span> <span class=n>generate_text_simple</span><span class=p>(</span>
</span></span><span class=line><span class=cl>	<span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>idx</span><span class=o>=</span><span class=n>text_to_token_ids</span><span class=p>(</span><span class=n>start_context</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>),</span>
</span></span><span class=line><span class=cl>	<span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>	<span class=n>context_size</span><span class=o>=</span><span class=n>GPT_CONFIG_124M</span><span class=p>[</span><span class=s2>&#34;context_length&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Output text:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>token_ids_to_text</span><span class=p>(</span><span class=n>token_ids</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>))</span>
</span></span></code></pre></div><p><em>5.1.2 Calculating the text generation loss</em></p><p>not just generating next token but also measuring the quality of the generated token</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://tyh382596868.github.io/posts/rtc/><span class=title>Â« Prev</span><br><span>Real-Time Execution of Action Chunking Flow Policies</span>
</a><a class=next href=https://tyh382596868.github.io/posts/score-based_generative_models/><span class=title>Next Â»</span><br><span>Score Based Generative Models</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer on x" href="https://x.com/intent/tweet/?text=Transformer&amp;url=https%3a%2f%2ftyh382596868.github.io%2fposts%2ftransformer%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftyh382596868.github.io%2fposts%2ftransformer%2f&amp;title=Transformer&amp;summary=Transformer&amp;source=https%3a%2f%2ftyh382596868.github.io%2fposts%2ftransformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftyh382596868.github.io%2fposts%2ftransformer%2f&title=Transformer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftyh382596868.github.io%2fposts%2ftransformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer on whatsapp" href="https://api.whatsapp.com/send?text=Transformer%20-%20https%3a%2f%2ftyh382596868.github.io%2fposts%2ftransformer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer on telegram" href="https://telegram.me/share/url?text=Transformer&amp;url=https%3a%2f%2ftyh382596868.github.io%2fposts%2ftransformer%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Transformer on ycombinator" href="https://news.ycombinator.com/submitlink?t=Transformer&u=https%3a%2f%2ftyh382596868.github.io%2fposts%2ftransformer%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://tyh382596868.github.io/>tyh's blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>